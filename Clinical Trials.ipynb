  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6de24e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T12:41:58.829211Z",
     "iopub.status.busy": "2025-10-08T12:41:58.828271Z",
     "iopub.status.idle": "2025-10-08T12:42:01.840675Z",
     "shell.execute_reply": "2025-10-08T12:42:01.839736Z"
    },
    "papermill": {
     "duration": 3.01931,
     "end_time": "2025-10-08T12:42:01.842135",
     "exception": false,
     "start_time": "2025-10-08T12:41:58.822825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: INITIAL DATA AUDIT AND FEATURE CONFIRMATION\n",
      "================================================================================\n",
      "\n",
      "[1] LOADING DATA...\n",
      "✓ Data loaded successfully!\n",
      "  Shape: 13748 rows × 11 columns\n",
      "\n",
      "[1.1] First 5 rows of the dataset:\n",
      "   index          NCT Sponsor  \\\n",
      "0      0  NCT00003305  Sanofi   \n",
      "1      1  NCT00003821  Sanofi   \n",
      "2      2  NCT00004025  Sanofi   \n",
      "3      3  NCT00005645  Sanofi   \n",
      "4      4  NCT00008281  Sanofi   \n",
      "\n",
      "                                               Title  \\\n",
      "0  A Phase II Trial of Aminopterin in Adults and ...   \n",
      "1  Phase II Trial of Aminopterin in Patients With...   \n",
      "2  Phase I/II Trial of the Safety, Immunogenicity...   \n",
      "3  Phase II Trial of ILX295501 Administered Orall...   \n",
      "4  A Multicenter, Open-Label, Randomized, Three-A...   \n",
      "\n",
      "                                             Summary  Start_Year  Start_Month  \\\n",
      "0  RATIONALE: Drugs used in chemotherapy use diff...        1997            7   \n",
      "1  RATIONALE: Drugs used in chemotherapy use diff...        1998            1   \n",
      "2  RATIONALE: Vaccines made from a person's white...        1999            3   \n",
      "3  RATIONALE: Drugs used in chemotherapy use diff...        1999            5   \n",
      "4  RATIONALE: Drugs used in chemotherapy use diff...        2000           10   \n",
      "\n",
      "             Phase  Enrollment          Status              Condition  \n",
      "0          Phase 2          75       Completed               Leukemia  \n",
      "1          Phase 2           0       Withdrawn  Endometrial Neoplasms  \n",
      "2  Phase 1/Phase 2          36  Unknown status               Melanoma  \n",
      "3          Phase 2           0       Withdrawn      Ovarian Neoplasms  \n",
      "4          Phase 3           0  Unknown status   Colorectal Neoplasms  \n",
      "\n",
      "================================================================================\n",
      "[2] COLUMN VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "[2.1] All columns in dataset:\n",
      "  1. index\n",
      "  2. NCT\n",
      "  3. Sponsor\n",
      "  4. Title\n",
      "  5. Summary\n",
      "  6. Start_Year\n",
      "  7. Start_Month\n",
      "  8. Phase\n",
      "  9. Enrollment\n",
      "  10. Status\n",
      "  11. Condition\n",
      "\n",
      "[2.2] Key columns identified:\n",
      "  ✓ Summary column: 'Summary'\n",
      "  ✓ Phase column: 'Phase'\n",
      "\n",
      "[2.3] Data types:\n",
      "index           int64\n",
      "NCT            object\n",
      "Sponsor        object\n",
      "Title          object\n",
      "Summary        object\n",
      "Start_Year      int64\n",
      "Start_Month     int64\n",
      "Phase          object\n",
      "Enrollment      int64\n",
      "Status         object\n",
      "Condition      object\n",
      "dtype: object\n",
      "\n",
      "================================================================================\n",
      "[3] MISSING DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[3.1] Columns with missing values:\n",
      "Column  Missing_Count  Missing_Percentage\n",
      " Phase            263            1.913006\n",
      " Title            144            1.047425\n",
      "\n",
      "  Summary column missing: 0.00%\n",
      "  Phase column missing: 1.91%\n",
      "\n",
      "[3.2] Generating missing data visualization...\n",
      "  ✓ Saved as 'missing_data_analysis.png'\n",
      "\n",
      "================================================================================\n",
      "[4] IDENTIFY POTENTIAL TARGET VARIABLES\n",
      "================================================================================\n",
      "\n",
      "[4.1] Potential target variables found:\n",
      "\n",
      "  • Status ('Status'):\n",
      "    Type: Categorical\n",
      "    Unique values: 9\n",
      "    Value counts:\n",
      "Status\n",
      "Completed                  10568\n",
      "Terminated                  1285\n",
      "Recruiting                   800\n",
      "Active, not recruiting       646\n",
      "Withdrawn                    291\n",
      "Not yet recruiting           108\n",
      "Unknown status                19\n",
      "Suspended                     16\n",
      "Enrolling by invitation       15\n",
      "\n",
      "  • Enrollment ('Enrollment'):\n",
      "    Type: Numerical\n",
      "    Statistics:\n",
      "count    13748.000000\n",
      "mean       440.783678\n",
      "std       1944.530768\n",
      "min          0.000000\n",
      "25%         40.000000\n",
      "50%        124.000000\n",
      "75%        365.000000\n",
      "max      84496.000000\n",
      "\n",
      "================================================================================\n",
      "[5] OVERALL DATASET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "[5.1] Basic statistics:\n",
      "               index          NCT Sponsor                    Title Summary  \\\n",
      "count   13748.000000        13748   13748                    13604   13748   \n",
      "unique           NaN        13748      10                    13434   13565   \n",
      "top              NaN  NCT00003305     GSK  Human Photoallergy Test  #NAME?   \n",
      "freq             NaN            1    2473                        7      11   \n",
      "mean     6873.500000          NaN     NaN                      NaN     NaN   \n",
      "std      3968.850085          NaN     NaN                      NaN     NaN   \n",
      "min         0.000000          NaN     NaN                      NaN     NaN   \n",
      "25%      3436.750000          NaN     NaN                      NaN     NaN   \n",
      "50%      6873.500000          NaN     NaN                      NaN     NaN   \n",
      "75%     10310.250000          NaN     NaN                      NaN     NaN   \n",
      "max     13747.000000          NaN     NaN                      NaN     NaN   \n",
      "\n",
      "          Start_Year   Start_Month    Phase    Enrollment     Status  \\\n",
      "count   13748.000000  13748.000000    13485  13748.000000      13748   \n",
      "unique           NaN           NaN        7           NaN          9   \n",
      "top              NaN           NaN  Phase 3           NaN  Completed   \n",
      "freq             NaN           NaN     4887           NaN      10568   \n",
      "mean     2009.155586      6.691155      NaN    440.783678        NaN   \n",
      "std         4.797615      3.486359      NaN   1944.530768        NaN   \n",
      "min      1984.000000      1.000000      NaN      0.000000        NaN   \n",
      "25%      2006.000000      4.000000      NaN     40.000000        NaN   \n",
      "50%      2009.000000      7.000000      NaN    124.000000        NaN   \n",
      "75%      2013.000000     10.000000      NaN    365.000000        NaN   \n",
      "max      2020.000000     12.000000      NaN  84496.000000        NaN   \n",
      "\n",
      "                        Condition  \n",
      "count                       13748  \n",
      "unique                        867  \n",
      "top     Diabetes Mellitus, Type 2  \n",
      "freq                          536  \n",
      "mean                          NaN  \n",
      "std                           NaN  \n",
      "min                           NaN  \n",
      "25%                           NaN  \n",
      "50%                           NaN  \n",
      "75%                           NaN  \n",
      "max                           NaN  \n",
      "\n",
      "[5.2] Memory usage:\n",
      "Index              132\n",
      "index           109984\n",
      "NCT             934864\n",
      "Sponsor         856432\n",
      "Title          3837617\n",
      "Summary        7083272\n",
      "Start_Year      109984\n",
      "Start_Month     109984\n",
      "Phase           875204\n",
      "Enrollment      109984\n",
      "Status          919128\n",
      "Condition      1025856\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "[6] RECOMMENDATIONS FOR NEXT STEPS\n",
      "================================================================================\n",
      "• Handle 1.91% missing values in Phase column (recommend imputing with 'Not Specified')\n",
      "• Verify date columns are in proper datetime format\n",
      "• Check for duplicate rows (based on NCT number)\n",
      "• Standardize text in Phase column (e.g., 'Phase 1' vs 'Phase I')\n",
      "\n",
      "================================================================================\n",
      "STEP 1 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Next: Proceed to Step 2 (EDA) or clean the data based on recommendations.\n",
      "Save cleaned data as 'clinical_trials_cleaned.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKYAAAJOCAYAAACN2Q8zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA83ElEQVR4nO3de5RWdaH/8c8AMoAjIBe5KKAgoijgLY28UaGDF8pLmWgK6tFMEE0xs4sgp7yVnbS8nJMFlmKGoSftoJIXDEIzDclUQgKxxFQUARUVeH5/uJhfE4iowFfl9Vpr1uLZez/7+e5nnu+aWW/23lNVqVQqAQAAAIANrEHpAQAAAACwcRKmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgCAYqqqqjJq1Kh1vt+tt946Q4YMWef7Zd2bO3duqqqqMnbs2PWyf58FAPhgE6YAgPdl7NixqaqqSlVVVaZMmbLK+kqlkk6dOqWqqiqHHHJIgRFuWCvfi6qqqjRq1CitWrXKbrvtltNPPz2PPfbYe97vq6++mlGjRuXee+9dd4P9N48//niqqqrSpEmTLFy4cL29DgDASo1KDwAA+Gho0qRJxo0bl7333rve8smTJ+fvf/97qqurV3nOa6+9lkaN1v2vIzNnzkyDBuX+/23//ffPcccdl0qlkpdffjmPPPJIrr322lx55ZW5+OKLc+aZZ77rfb766qs5//zzkyT9+vVbxyN+y3XXXZf27dvnpZdeyk033ZT/+I//WC+vsyGV/iwAAGsmTAEA68RBBx2U8ePH5/LLL68Xm8aNG5fddtstL7zwwirPadKkyXoZy+oi2Ia03Xbb5Ytf/GK9ZRdddFEGDhyYs846K9tvv30OOuigQqNbvUqlknHjxuXoo4/OnDlzcv31138kwlTpzwIAsGb++wgAWCcGDRqUBQsWZNKkSXXL3njjjdx00005+uijV/ucf7/H1OLFi3PGGWdk6623TnV1dbbYYovsv//+efjhh+u2mTVrVo444oi0b98+TZo0yVZbbZWjjjoqL7/8ct02/35foZWXG06dOjVnnnlm2rZtm0033TSHHXZYnn/++XpjWrFiRUaNGpWOHTumWbNm+eQnP5nHHnvsfd+rqHXr1vnFL36RRo0a5Tvf+U699+i8887LbrvtlhYtWmTTTTfNPvvsk3vuuadum7lz56Zt27ZJkvPPP7/uUsGV792MGTMyZMiQdO3aNU2aNEn79u1zwgknZMGCBWs9vqlTp2bu3Lk56qijctRRR+W+++7L3//+91W223rrrXPIIYdkypQp2WOPPdKkSZN07do1P/vZz+pt9+KLL2bEiBHp1atXampq0rx58xx44IF55JFH1jiOMWPGpKqqKn/6059WWXfBBRekYcOG+cc//pHkvX0W3nzzzZx//vnp3r17mjRpktatW2fvvfeu97kFADYcYQoAWCe23nrr9O3bNzfccEPdsokTJ+bll1/OUUcdtVb7OOWUU3LVVVfliCOOyJVXXpkRI0akadOmefzxx5O8FXFqa2tz//3357TTTssVV1yRk08+OX/729/W6p5Ip512Wh555JGMHDkyX/7yl3Prrbdm2LBh9bY599xzc/7552f33XfPd7/73XTv3j21tbV55ZVX1v7NeBudO3fOfvvtl/vvvz+LFi1KkixatCjXXHNN+vXrl4svvjijRo3K888/n9ra2kyfPj1J0rZt21x11VVJksMOOyw///nP8/Of/zyHH354kmTSpEn529/+luOPPz4//OEPc9RRR+UXv/hFDjrooFQqlbUa2/XXX59u3brlYx/7WAYOHJhmzZrV+17+qyeffDKf+9znsv/+++fSSy/N5ptvniFDhuQvf/lL3TZ/+9vfcsstt+SQQw7J97///Zx99tn585//nP322y/PPPPM247jc5/7XJo2bZrrr79+tWPs169fttxyy/f8WRg1alTOP//8fPKTn8yPfvSjfOMb30jnzp3rxU8AYAOqAAC8D2PGjKkkqTz44IOVH/3oR5XNNtus8uqrr1YqlUrl85//fOWTn/xkpVKpVLp06VI5+OCD6z03SWXkyJF1j1u0aFEZOnTo277Wn/70p0qSyvjx49c4pi5dulQGDx68yhj79+9fWbFiRd3yr3zlK5WGDRtWFi5cWKlUKpVnn3220qhRo8qhhx5ab3+jRo2qJKm3z7eTZI3HcPrpp1eSVB555JFKpVKpLFu2rPL666/X2+all16qtGvXrnLCCSfULXv++edXeb9WWvl+/6sbbrihkqRy3333veOY33jjjUrr1q0r3/jGN+qWHX300ZU+ffqssm2XLl1W2e9zzz1Xqa6urpx11ll1y5YuXVpZvnx5vefOmTOnUl1dXRk9enS9ZUkqY8aMqVs2aNCgSseOHes9/+GHH6633Xv9LPTp02eVzyEAUI4zpgCAdebII4/Ma6+9lttuuy2LFy/Obbfd9raX8a1Oy5Yt88ADD7ztGTUtWrRIktxxxx159dVX3/X4Tj755FRVVdU93meffbJ8+fI89dRTSZK77rory5Yty6mnnlrveaeddtq7fq23U1NTk+StyxaTpGHDhmncuHGSty4jfPHFF7Ns2bLsvvvua30WT9OmTev+vXTp0rzwwgv5+Mc/niRrtY+JEydmwYIFGTRoUN2yQYMG5ZFHHql3FtRKPXv2zD777FP3uG3btunRo0f+9re/1S2rrq6uu+n48uXLs2DBgtTU1KRHjx7vOKbjjjsuzzzzTL3LGa+//vo0bdo0RxxxRJL3/llo2bJl/vKXv2TWrFlr/RwAYP0RpgCAdaZt27bp379/xo0blwkTJmT58uX53Oc+t9bPv+SSS/Loo4+mU6dO2WOPPTJq1Kh6sWObbbbJmWeemWuuuSZt2rRJbW1trrjiinr3FFqTzp0713u8+eabJ0leeumlJKkLVNtuu2297Vq1alW37fu1ZMmSJMlmm21Wt+zaa69N79696+551LZt2/zmN79Z6+N68cUXc/rpp6ddu3Zp2rRp2rZtm2222SZJ1mof1113XbbZZptUV1fnySefzJNPPplu3bqlWbNmq72k7t/fx+St93Ll+5i8Fdn+67/+K927d091dXXatGmTtm3bZsaMGe84pv333z8dOnSoe+0VK1bkhhtuyGc/+9m69+29fhZGjx6dhQsXZrvttkuvXr1y9tlnZ8aMGe/4HgEA64cwBQCsU0cffXQmTpyYq6++OgceeGBatmy51s898sgj87e//S0//OEP07Fjx3z3u9/NjjvumIkTJ9Ztc+mll2bGjBn5+te/ntdeey3Dhw/PjjvuuNobdf+7hg0brnZ5ZS3vw7QuPProo2nYsGFdOLruuusyZMiQdOvWLT/5yU9y++23Z9KkSfnUpz6VFStWrNU+jzzyyPz4xz/OKaeckgkTJuTOO+/M7bffniTvuI9Fixbl1ltvzZw5c9K9e/e6r549e+bVV1/NuHHjVnl/1uZ9vOCCC3LmmWdm3333zXXXXZc77rgjkyZNyo477viOY2rYsGGOPvro/OpXv8rSpUtzzz335JlnnlnlLx2+l8/Cvvvum9mzZ+enP/1pdtppp1xzzTXZddddc80116xxTADA+iFMAQDr1GGHHZYGDRrk/vvvf1eX8a3UoUOHnHrqqbnlllsyZ86ctG7dut5fsUuSXr165Zvf/Gbuu+++/O53v8s//vGPXH311e977F26dEny1s29/9WCBQvqnQ30Xs2bNy+TJ09O37596878uemmm9K1a9dMmDAhxx57bGpra9O/f/8sXbq03nP/9RLEf/XSSy/lrrvuyte+9rWcf/75Oeyww7L//vuna9euazWmCRMmZOnSpbnqqqsyfvz4el/f/va389RTT2Xq1Knv+lhvuummfPKTn8xPfvKTHHXUUTnggAPSv3//tbpJffLW5Xwro9n111+ftm3bpra2dpXt3stnoVWrVjn++ONzww035Omnn07v3r3r/XVIAGDDaVR6AADAR0tNTU2uuuqqzJ07NwMHDlzr5y1fvjxLliypu3dQkmyxxRbp2LFjXn/99SRvnd3TrFmzNGr0/3+F6dWrVxo0aFC3zfvx6U9/Oo0aNcpVV12V/fffv275j370o/e97xdffDGDBg3K8uXL841vfKNu+cqzjyqVSl18euCBBzJt2rR6l8w1a9YsSVYJO//6/H/1gx/8YK3Gdd1116Vr16455ZRTVln3+uuv56KLLsr111+fvffee63296/j+vcxjR8/Pv/4xz9WuVRydXr37p3evXvnmmuuyf3335/BgwfX+76/18/CggUL0rp167rHNTU12XbbbfP000+/m8MDANYRYQoAWOcGDx78rp+zePHibLXVVvnc5z6XPn36pKamJr/97W/z4IMP5tJLL02S3H333Rk2bFg+//nPZ7vttsuyZcvy85//PA0bNqy7Kfb70a5du5x++um59NJL85nPfCYDBgzII488kokTJ6ZNmzZve9bSv/vrX/+a6667LpVKJYsWLcojjzyS8ePHZ8mSJfn+97+fAQMG1G17yCGHZMKECTnssMNy8MEHZ86cObn66qvTs2fPuvtRJW/d4Lxnz5658cYbs91226VVq1bZaaedstNOO2XffffNJZdckjfffDNbbrll7rzzzsyZM+cdx7nyBuPDhw9f7frq6urU1tZm/Pjxufzyy7PJJpus1fGvPK7Ro0fn+OOPzyc+8Yn8+c9/zvXXX7/WZ3Ilb501NWLEiCRZ5TK+9/pZ6NmzZ/r165fddtstrVq1yh//+MfcdNNNGTZs2FqPCwBYd4QpAOADoVmzZjn11FNz5513ZsKECVmxYkW23XbbXHnllfnyl7+cJOnTp09qa2tz66235h//+EeaNWuWPn36ZOLEiXV/he79uvjii9OsWbP8+Mc/zm9/+9v07ds3d955Z/bee+80adJkrfYxadKkTJo0KQ0aNEjz5s2zzTbbZPDgwTn55JPTs2fPetsOGTIkzz77bP77v/87d9xxR3r27Jnrrrsu48ePz7333ltv22uuuSannXZavvKVr+SNN97IyJEjs9NOO2XcuHE57bTTcsUVV6RSqeSAAw7IxIkT07FjxzWO8xe/+EVWrFixxjPbBg4cmF/96leZOHFiPvOZz6zV8SfJ17/+9bzyyisZN25cbrzxxuy66675zW9+k6997WtrvY9jjjkm55xzTrp165Y99tij3rr3+lkYPnx4fv3rX+fOO+/M66+/ni5duuTb3/52zj777LUeFwCw7lRVNuTdPgEAPoQWLlyYzTffPN/+9rfrXYbH+vXCCy+kQ4cOOe+88/Ktb32r9HAAgPXAzc8BAP7Fa6+9tsqylfdr6tev34YdzEZu7NixWb58eY499tjSQwEA1hOX8gEA/Isbb7wxY8eOzUEHHZSamppMmTIlN9xwQw444IDstddepYe3Ubj77rvz2GOP5Tvf+U4OPfTQbL311qWHBACsJy7lAwD4Fw8//HC++tWvZvr06Vm0aFHatWuXI444It/+9rdTU1NTengbhX79+uX3v/999tprr1x33XXZcsstSw8JAFhPhCkAAAAAinCPKQAAAACKEKYAAAAAKGKju/n5ihUr8swzz2SzzTZLVVVV6eEAAAAAfKRUKpUsXrw4HTt2TIMGaz4naqMLU88880w6depUehgAAAAAH2lPP/10ttpqqzVus9GFqc022yzJW29O8+bNC48GAAAA4KNl0aJF6dSpU12DWZONLkytvHyvefPmwhQAAADAerI2t1By83MAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIhqVHkApO428Iw2qm5UeBgAAALCRm3vRwaWHUIwzpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIpYZ2Fq7Nixadmy5braHQAAAAAfce8qTA0ZMiRVVVWpqqpK48aNs+2222b06NFZtmzZ+hofAAAAAB9Rjd7tEwYMGJAxY8bk9ddfz//93/9l6NCh2WSTTdKhQ4f1MT4AAAAAPqLe9aV81dXVad++fbp06ZIvf/nL6d+/f37961/Xrb/jjjuyww47pKamJgMGDMj8+fPr1j344IPZf//906ZNm7Ro0SL77bdfHn744br1lUolo0aNSufOnVNdXZ2OHTtm+PDhdetff/31jBgxIltuuWU23XTT7Lnnnrn33nvf46EDAAAAUNL7vsdU06ZN88YbbyRJXn311Xzve9/Lz3/+89x3332ZN29eRowYUbft4sWLM3jw4EyZMiX3339/unfvnoMOOiiLFy9OkvzqV7/Kf/3Xf+W///u/M2vWrNxyyy3p1atX3fOHDRuWadOm5Re/+EVmzJiRz3/+8xkwYEBmzZr1tuN7/fXXs2jRonpfAAAAAJT3ri/lW6lSqeSuu+7KHXfckdNOOy1J8uabb+bqq69Ot27dkrwVkkaPHl33nE996lP19vE///M/admyZSZPnpxDDjkk8+bNS/v27dO/f/9ssskm6dy5c/bYY48kybx58zJmzJjMmzcvHTt2TJKMGDEit99+e8aMGZMLLrhgteO88MILc/7557/XwwQAAABgPXnXZ0zddtttqampSZMmTXLggQfmC1/4QkaNGpUkadasWV2USpIOHTrkueeeq3v8z3/+MyeddFK6d++eFi1apHnz5lmyZEnmzZuXJPn85z+f1157LV27ds1JJ52Um2++ue7G6n/+85+zfPnybLfddqmpqan7mjx5cmbPnv224z333HPz8ssv1309/fTT7/aQAQAAAFgP3vUZU5/85Cdz1VVXpXHjxunYsWMaNfr/u9hkk03qbVtVVZVKpVL3ePDgwVmwYEEuu+yydOnSJdXV1enbt2/dpYCdOnXKzJkz89vf/jaTJk3Kqaeemu9+97uZPHlylixZkoYNG+ahhx5Kw4YN671OTU3N2463uro61dXV7/YwAQAAAFjP3nWY2nTTTbPtttu+pxebOnVqrrzyyhx00EFJkqeffjovvPBCvW2aNm2agQMHZuDAgRk6dGi23377/PnPf84uu+yS5cuX57nnnss+++zznl4fAAAAgA+O93yPqfeie/fu+fnPf57dd989ixYtytlnn52mTZvWrR87dmyWL1+ePffcM82aNct1112Xpk2bpkuXLmndunWOOeaYHHfccbn00kuzyy675Pnnn89dd92V3r175+CDD96QhwIAAADA+/S+/yrfu/GTn/wkL730Unbdddcce+yxGT58eLbYYou69S1btsyPf/zj7LXXXundu3d++9vf5tZbb03r1q2TJGPGjMlxxx2Xs846Kz169Mihhx6aBx98MJ07d96QhwEAAADAOlBV+debQG0EFi1alBYtWqTTGb9Mg+pmpYcDAAAAbOTmXvTRugpsZXt5+eWX07x58zVuu0HPmAIAAACAlYQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKKJR6QGU8uj5tWnevHnpYQAAAABstJwxBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARTQqPYBSdhp5RxpUNys9DACAD4S5Fx1ceggAwEbIGVMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBFFw9SQIUNy6KGHrnGbe++9N1VVVVm4cOEGGRMAAAAAG0aj9bXjqqqqNa4fOXJkLrvsslQqlbpl/fr1y84775wf/OAH62tYAAAAAHxArLcwNX/+/Lp/33jjjTnvvPMyc+bMumU1NTWpqalZXy8PAAAAwAfceruUr3379nVfLVq0SFVVVb1lNTU19S7lGzJkSCZPnpzLLrssVVVVqaqqyty5c1e77ylTpmSfffZJ06ZN06lTpwwfPjyvvPLK+joUAAAAANaDD8zNzy+77LL07ds3J510UubPn5/58+enU6dOq2w3e/bsDBgwIEcccURmzJiRG2+8MVOmTMmwYcMKjBoAAACA92q9Xcr3brVo0SKNGzdOs2bN0r59+7fd7sILL8wxxxyTM844I0nSvXv3XH755dlvv/1y1VVXpUmTJvW2f/311/P666/XPV60aNF6GT8AAAAA784H5oyptfXII49k7NixdfeoqqmpSW1tbVasWJE5c+assv2FF16YFi1a1H2t7iwsAAAAADa8D8wZU2tryZIl+dKXvpThw4evsq5z586rLDv33HNz5pln1j1etGiROAUAAADwAfCBClONGzfO8uXL17jNrrvumsceeyzbbrvtWu2zuro61dXV62J4AAAAAKxDH6hL+bbeeus88MADmTt3bl544YWsWLFilW3OOeec/P73v8+wYcMyffr0zJo1K//7v//r5ucAAAAAHzIfqDA1YsSINGzYMD179kzbtm0zb968Vbbp3bt3Jk+enL/+9a/ZZ599sssuu+S8885Lx44dC4wYAAAAgPeqqlKpVEoPYkNatGjRWzdBP+OXaVDdrPRwAAA+EOZedHDpIQAAHxEr28vLL7+c5s2br3HbD9QZUwAAAABsPIQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKAIYQoAAACAIoQpAAAAAIoQpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQAAAACKEKYAAAAAKEKYAgAAAKCIRqUHUMqj59emefPmpYcBAAAAsNFyxhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABFCFMAAAAAFCFMAQAAAFCEMAUAAABAEcIUAAAAAEU0Kj2ADa1SqSRJFi1aVHgkAAAAAB89K5vLygazJhtdmFqwYEGSpFOnToVHAgAAAPDRtXjx4rRo0WKN22x0YapVq1ZJknnz5r3jmwMbs0WLFqVTp055+umn07x589LDgQ8k8wTWjrkC78w8gbVjrnw4VCqVLF68OB07dnzHbTe6MNWgwVu31WrRooUPMayF5s2bmyvwDswTWDvmCrwz8wTWjrnywbe2JwO5+TkAAAAARQhTAAAAABSx0YWp6urqjBw5MtXV1aWHAh9o5gq8M/ME1o65Au/MPIG1Y6589FRV1uZv9wEAAADAOrbRnTEFAAAAwAeDMAUAAABAEcIUAAAAAEVsdGHqiiuuyNZbb50mTZpkzz33zB/+8IfSQ4JiRo0alaqqqnpf22+/fd36pUuXZujQoWndunVqampyxBFH5J///GfBEcOGcd9992XgwIHp2LFjqqqqcsstt9RbX6lUct5556VDhw5p2rRp+vfvn1mzZtXb5sUXX8wxxxyT5s2bp2XLljnxxBOzZMmSDXgUsH690zwZMmTIKj9jBgwYUG8b84SPugsvvDAf+9jHstlmm2WLLbbIoYcempkzZ9bbZm1+35o3b14OPvjgNGvWLFtssUXOPvvsLFu2bEMeCqxXazNX+vXrt8rPlVNOOaXeNubKh9NGFaZuvPHGnHnmmRk5cmQefvjh9OnTJ7W1tXnuuedKDw2K2XHHHTN//vy6rylTptSt+8pXvpJbb70148ePz+TJk/PMM8/k8MMPLzha2DBeeeWV9OnTJ1dcccVq119yySW5/PLLc/XVV+eBBx7Ipptumtra2ixdurRum2OOOSZ/+ctfMmnSpNx222257777cvLJJ2+oQ4D17p3mSZIMGDCg3s+YG264od5684SPusmTJ2fo0KG5//77M2nSpLz55ps54IAD8sorr9Rt806/by1fvjwHH3xw3njjjfz+97/Ptddem7Fjx+a8884rcUiwXqzNXEmSk046qd7PlUsuuaRunbnyIVbZiOyxxx6VoUOH1j1evnx5pWPHjpULL7yw4KignJEjR1b69Omz2nULFy6sbLLJJpXx48fXLXv88ccrSSrTpk3bQCOE8pJUbr755rrHK1asqLRv377y3e9+t27ZwoULK9XV1ZUbbrihUqlUKo899lglSeXBBx+s22bixImVqqqqyj/+8Y8NNnbYUP59nlQqlcrgwYMrn/3sZ9/2OeYJG6PnnnuukqQyefLkSqWydr9v/d///V+lQYMGlWeffbZum6uuuqrSvHnzyuuvv75hDwA2kH+fK5VKpbLffvtVTj/99Ld9jrny4bXRnDH1xhtv5KGHHkr//v3rljVo0CD9+/fPtGnTCo4Mypo1a1Y6duyYrl275phjjsm8efOSJA899FDefPPNenNm++23T+fOnc0ZNmpz5szJs88+W29utGjRInvuuWfd3Jg2bVpatmyZ3XffvW6b/v37p0GDBnnggQc2+JihlHvvvTdbbLFFevTokS9/+ctZsGBB3TrzhI3Ryy+/nCRp1apVkrX7fWvatGnp1atX2rVrV7dNbW1tFi1alL/85S8bcPSw4fz7XFnp+uuvT5s2bbLTTjvl3HPPzauvvlq3zlz58GpUegAbygsvvJDly5fX+5AmSbt27fLEE08UGhWUteeee2bs2LHp0aNH5s+fn/PPPz/77LNPHn300Tz77LNp3LhxWrZsWe857dq1y7PPPltmwPABsPLzv7qfJyvXPfvss9liiy3qrW/UqFFatWpl/rDRGDBgQA4//PBss802mT17dr7+9a/nwAMPzLRp09KwYUPzhI3OihUrcsYZZ2SvvfbKTjvtlCRr9fvWs88+u9qfOSvXwUfN6uZKkhx99NHp0qVLOnbsmBkzZuScc87JzJkzM2HChCTmyofZRhOmgFUdeOCBdf/u3bt39txzz3Tp0iW//OUv07Rp04IjA+DD7qijjqr7d69evdK7d+9069Yt9957bz796U8XHBmUMXTo0Dz66KP17ucJrOrt5sq/3oOwV69e6dChQz796U9n9uzZ6dat24YeJuvQRnMpX5s2bdKwYcNV/sLFP//5z7Rv377QqOCDpWXLltluu+3y5JNPpn379nnjjTeycOHCetuYM2zsVn7+1/TzpH379qv8YY1ly5blxRdfNH/YaHXt2jVt2rTJk08+mcQ8YeMybNiw3Hbbbbnnnnuy1VZb1S1fm9+32rdvv9qfOSvXwUfJ282V1dlzzz2TpN7PFXPlw2mjCVONGzfObrvtlrvuuqtu2YoVK3LXXXelb9++BUcGHxxLlizJ7Nmz06FDh+y2227ZZJNN6s2ZmTNnZt68eeYMG7Vtttkm7du3rzc3Fi1alAceeKBubvTt2zcLFy7MQw89VLfN3XffnRUrVtT9EgUbm7///e9ZsGBBOnTokMQ8YeNQqVQybNiw3Hzzzbn77ruzzTbb1Fu/Nr9v9e3bN3/+85/rhdxJkyalefPm6dmz54Y5EFjP3mmurM706dOTpN7PFXPlw2mjupTvzDPPzODBg7P77rtnjz32yA9+8IO88sorOf7440sPDYoYMWJEBg4cmC5duuSZZ57JyJEj07BhwwwaNCgtWrTIiSeemDPPPDOtWrVK8+bNc9ppp6Vv3775+Mc/XnrosF4tWbKk7n/fkrdueD59+vS0atUqnTt3zhlnnJFvf/vb6d69e7bZZpt861vfSseOHXPooYcmSXbYYYcMGDAgJ510Uq6++uq8+eabGTZsWI466qh07Nix0FHBurWmedKqVaucf/75OeKII9K+ffvMnj07X/3qV7PtttumtrY2iXnCxmHo0KEZN25c/vd//zebbbZZ3X1uWrRokaZNm67V71sHHHBAevbsmWOPPTaXXHJJnn322Xzzm9/M0KFDU11dXfLwYJ15p7kye/bsjBs3LgcddFBat26dGTNm5Ctf+Ur23Xff9O7dO4m58qFW+s8Cbmg//OEPK507d640bty4sscee1Tuv//+0kOCYr7whS9UOnToUGncuHFlyy23rHzhC1+oPPnkk3XrX3vttcqpp55a2XzzzSvNmjWrHHbYYZX58+cXHDFsGPfcc08lySpfgwcPrlQqlcqKFSsq3/rWtyrt2rWrVFdXVz796U9XZs6cWW8fCxYsqAwaNKhSU1NTad68eeX444+vLF68uMDRwPqxpnny6quvVg444IBK27ZtK5tsskmlS5culZNOOqnen/CuVMwTPvpWN0eSVMaMGVO3zdr8vjV37tzKgQceWGnatGmlTZs2lbPOOqvy5ptvbuCjgfXnnebKvHnzKvvuu2+lVatWlerq6sq2225bOfvssysvv/xyvf2YKx9OVZVKpbIhQxgAAAAAJBvRPaYAAAAA+GARpgAAAAAoQpgCAAAAoAhhCgAAAIAihCkAAAAAihCmAAAAAChCmAIAAACgCGEKAAAAgCKEKQDgQ2/u3LmpqqrK9OnTSw+lzhNPPJGPf/zjadKkSXbeeed1ss9+/frljDPOeN/7GTJkSA499ND3vZ8NYV0dMwDwwSRMAQDv25AhQ1JVVZWLLrqo3vJbbrklVVVVhUZV1siRI7Pppptm5syZueuuu1a7zcr37ZRTTlll3dChQ1NVVZUhQ4bULZswYUL+8z//832P7bLLLsvYsWPf937WZODAgRkwYMBq1/3ud79LVVVVZsyYsV7HAAB88AlTAMA60aRJk1x88cV56aWXSg9lnXnjjTfe83Nnz56dvffeO126dEnr1q3fdrtOnTrlF7/4RV577bW6ZUuXLs24cePSuXPnetu2atUqm2222Xse00otWrRIy5Yt3/d+1uTEE0/MpEmT8ve//32VdWPGjMnuu++e3r17r9cxAAAffMIUALBO9O/fP+3bt8+FF174ttuMGjVqlcvafvCDH2Trrbeue7zyMrMLLrgg7dq1S8uWLTN69OgsW7YsZ599dlq1apWtttoqY8aMWWX/TzzxRD7xiU+kSZMm2WmnnTJ58uR66x999NEceOCBqampSbt27XLsscfmhRdeqFvfr1+/DBs2LGeccUbatGmT2tra1R7HihUrMnr06Gy11Vaprq7OzjvvnNtvv71ufVVVVR566KGMHj06VVVVGTVq1Nu+J7vuums6deqUCRMm1C2bMGFCOnfunF122aXetv9+WduVV16Z7t27p0mTJmnXrl0+97nP1a276aab0qtXrzRt2jStW7dO//7988orr9R7j/91v8OHD89Xv/rVtGrVKu3bt19lzE888UT23nvvNGnSJD179sxvf/vbVFVV5ZZbblntcR1yyCFp27btKmdmLVmyJOPHj8+JJ56YBQsWZNCgQdlyyy3TrFmz9OrVKzfccMPbvldJVvuaLVu2rPc6Tz/9dI488si0bNkyrVq1ymc/+9nMnTu3bv29996bPfbYI5tuumlatmyZvfbaK0899dQaXxcAWD+EKQBgnWjYsGEuuOCC/PCHP1ztWTLvxt13351nnnkm9913X77//e9n5MiROeSQQ7L55pvngQceyCmnnJIvfelLq7zO2WefnbPOOit/+tOf0rdv3wwcODALFixIkixcuDCf+tSnsssuu+SPf/xjbr/99vzzn//MkUceWW8f1157bRo3bpypU6fm6quvXu34Lrvsslx66aX53ve+lxkzZqS2tjaf+cxnMmvWrCTJ/Pnzs+OOO+ass87K/PnzM2LEiDUe7wknnFAvtP30pz/N8ccfv8bn/PGPf8zw4cMzevTozJw5M7fffnv23XffutcfNGhQTjjhhDz++OO59957c/jhh6dSqbzt/q699tpsuummeeCBB3LJJZdk9OjRmTRpUpJk+fLlOfTQQ9OsWbM88MAD+Z//+Z984xvfWOP4GjVqlOOOOy5jx46t97rjx4/P8uXLM2jQoCxdujS77bZbfvOb3+TRRx/NySefnGOPPTZ/+MMf1rjvNXnzzTdTW1ubzTbbLL/73e8yderU1NTUZMCAAXnjjTeybNmyHHroodlvv/0yY8aMTJs2LSeffPJGe8kpAJQmTAEA68xhhx2WnXfeOSNHjnxf+2nVqlUuv/zy9OjRIyeccEJ69OiRV199NV//+tfTvXv3nHvuuWncuHGmTJlS73nDhg3LEUcckR122CFXXXVVWrRokZ/85CdJkh/96EfZZZddcsEFF2T77bfPLrvskp/+9Ke555578te//rVuH927d88ll1ySHj16pEePHqsd3/e+972cc845Oeqoo9KjR49cfPHF2XnnnfODH/wgSdK+ffs0atQoNTU1ad++fWpqatZ4vF/84hczZcqUPPXUU3nqqacyderUfPGLX1zjc+bNm5dNN900hxxySLp06ZJddtklw4cPT/JWmFq2bFkOP/zwbL311unVq1dOPfXUNY6jd+/eGTlyZLp3757jjjsuu+++e929sSZNmpTZs2fnZz/7Wfr06ZO999473/nOd9Y4vuSt4DZ79ux6Z66NGTMmRxxxRFq0aJEtt9wyI0aMyM4775yuXbvmtNNOy4ABA/LLX/7yHff9dm688casWLEi11xzTXr16pUddtghY8aMybx583Lvvfdm0aJFefnll3PIIYekW7du2WGHHTJ48OBVLpsEADYMYQoAWKcuvvjiXHvttXn88cff8z523HHHNGjw/39NadeuXXr16lX3uGHDhmndunWee+65es/r27dv3b8bNWqU3XffvW4cjzzySO65557U1NTUfW2//fZJ3rof1Eq77bbbGse2aNGiPPPMM9lrr73qLd9rr73e8zG3bds2Bx98cMaOHZsxY8bk4IMPTps2bdb4nP333z9dunRJ165dc+yxx+b666/Pq6++miTp06dPPv3pT6dXr175/Oc/nx//+MfveO+vf7/fU4cOHere35kzZ6ZTp05p37593fo99tjjHY9r++23zyc+8Yn89Kc/TZI8+eST+d3vfpcTTzwxyVtnYv3nf/5nevXqlVatWqWmpiZ33HFH5s2b9477fjuPPPJInnzyyWy22WZ13+dWrVpl6dKlmT17dlq1apUhQ4aktrY2AwcOzGWXXZb58+e/59cDAN4fYQoAWKf23Xff1NbW5txzz11lXYMGDVa5nOzNN99cZbtNNtmk3uOqqqrVLluxYsVaj2vJkiUZOHBgpk+fXu9r1qxZdZfAJcmmm2661vtcl0444YSMHTs21157bU444YR33H6zzTbLww8/nBtuuCEdOnTIeeedlz59+mThwoVp2LBhJk2alIkTJ6Znz5754Q9/mB49emTOnDlvu7/3+/6+nRNPPDG/+tWvsnjx4owZMybdunXLfvvtlyT57ne/m8suuyznnHNO7rnnnkyfPj21tbVrvOl8VVXVGj9DS5YsyW677bbK9/mvf/1rjj766CRvnbU1bdq0fOITn8iNN96Y7bbbLvfff//7PlYA4N0TpgCAde6iiy7KrbfemmnTptVb3rZt2zz77LP1wsL06dPX2ev+a1xYtmxZHnrooeywww5J3rrJ+F/+8pdsvfXW2Xbbbet9vZsY1bx583Ts2DFTp06tt3zq1Knp2bPnex77ynsgrbxH0tpo1KhR+vfvn0suuSQzZszI3Llzc/fddyd5K+DstddeOf/88/OnP/0pjRs3zs033/yextajR488/fTT+ec//1m37MEHH1yr5x555JFp0KBBxo0bl5/97Gc54YQT6u7nNHXq1Hz2s5/NF7/4xfTp0yddu3atd1nl6rRt27beGU6zZs2qO1Mseev7PGvWrGyxxRarfJ9btGhRt90uu+ySc889N7///e+z0047Zdy4cWt1PADAuiVMAQDrXK9evXLMMcfk8ssvr7e8X79+ef7553PJJZdk9uzZueKKKzJx4sR19rpXXHFFbr755jzxxBMZOnRoXnrppbqzj4YOHZoXX3wxgwYNyoMPPpjZs2fnjjvuyPHHH5/ly5e/q9c5++yzc/HFF+fGG2/MzJkz87WvfS3Tp0/P6aef/p7H3rBhwzz++ON57LHH0rBhw3fc/rbbbsvll1+e6dOn56mnnsrPfvazrFixIj169MgDDzyQCy64IH/84x8zb968TJgwIc8//3xdpHu39t9//3Tr1i2DBw/OjBkzMnXq1Hzzm99Mkne8aXhNTU2+8IUv5Nxzz838+fMzZMiQunXdu3fPpEmT8vvf/z6PP/54vvSlL9WLX6vzqU99Kj/60Y/ypz/9KX/84x9zyimn1Dvb65hjjkmbNm3y2c9+Nr/73e8yZ86c3HvvvRk+fHj+/ve/Z86cOTn33HMzbdq0PPXUU7nzzjsza9as9/zeAADvjzAFAKwXo0ePXuVSsB122CFXXnllrrjiivTp0yd/+MMf3vEv1r0bF110US666KL06dMnU6ZMya9//eu6ezWtPMtp+fLlOeCAA9KrV6+cccYZadmyZb37Wa2N4cOH58wzz8xZZ52VXr165fbbb8+vf/3rdO/e/X2Nv3nz5mnevPlabduyZctMmDAhn/rUp7LDDjvk6quvzg033JAdd9wxzZs3z3333ZeDDjoo2223Xb75zW/m0ksvzYEHHviextWwYcPccsstWbJkST72sY/lP/7jP+r+Kl+TJk3e8fknnnhiXnrppdTW1qZjx451y7/5zW9m1113TW1tbfr165f27dvn0EMPXeO+Lr300nTq1Cn77LNPjj766IwYMSLNmjWrW9+sWbPcd9996dy5cw4//PDssMMOOfHEE7N06dI0b948zZo1yxNPPJEjjjgi2223XU4++eQMHTo0X/rSl97TewMAvD9VlTX93WAAAFiNqVOnZu+9986TTz6Zbt26lR4OAPAhJUwBAPCObr755tTU1KR79+558sknc/rpp2fzzTfPlClTSg8NAPgQa1R6AAAAfPAtXrw455xzTubNm5c2bdqkf//+ufTSS0sPCwD4kHPGFAAAAABFuPk5AAAAAEUIUwAAAAAUIUwBAAAAUIQwBQAAAEARwhQAAAAARQhTAAAAABQhTAEAAABQhDAFAAAAQBHCFAAAAABF/D/FrqyuCnrCqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: INITIAL DATA AUDIT AND FEATURE CONFIRMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD AND VERIFY DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "try:\n",
    "    df = pd.read_csv('/kaggle/input/aero-birdseye-data/AERO-BirdsEye-Data.csv')\n",
    "    print(f\"✓ Data loaded successfully!\")\n",
    "    print(f\"  Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Error: File 'AERO-BirdsEye-Data.csv' not found!\")\n",
    "    print(\"  Please ensure the file is in the current directory.\")\n",
    "    exit()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n[1.1] First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# ============================================================================\n",
    "# 2. VERIFY COLUMN NAMES AND DATA TYPES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] COLUMN VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[2.1] All columns in dataset:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Check for key columns\n",
    "key_columns = {\n",
    "    'summary': ['Summary', 'summary', 'SUMMARY'],\n",
    "    'phases': ['Phase', 'phase', 'PHASE', 'phases']\n",
    "}\n",
    "\n",
    "found_columns = {}\n",
    "for key, possible_names in key_columns.items():\n",
    "    for name in possible_names:\n",
    "        if name in df.columns:\n",
    "            found_columns[key] = name\n",
    "            break\n",
    "\n",
    "print(\"\\n[2.2] Key columns identified:\")\n",
    "if 'summary' in found_columns:\n",
    "    print(f\"  ✓ Summary column: '{found_columns['summary']}'\")\n",
    "else:\n",
    "    print(\"  ✗ Summary column not found! Please check column names.\")\n",
    "    \n",
    "if 'phases' in found_columns:\n",
    "    print(f\"  ✓ Phase column: '{found_columns['phases']}'\")\n",
    "else:\n",
    "    print(\"  ✗ Phase column not found! Please check column names.\")\n",
    "\n",
    "# Display data types\n",
    "print(\"\\n[2.3] Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. HANDLE MISSING DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] MISSING DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Percentage', ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\n[3.1] Columns with missing values:\")\n",
    "if len(missing_data) > 0:\n",
    "    print(missing_data.to_string(index=False))\n",
    "    \n",
    "    # Check critical columns\n",
    "    if 'summary' in found_columns:\n",
    "        summary_col = found_columns['summary']\n",
    "        missing_pct = (df[summary_col].isnull().sum() / len(df)) * 100\n",
    "        print(f\"\\n  Summary column missing: {missing_pct:.2f}%\")\n",
    "        if missing_pct > 5:\n",
    "            print(f\"  ⚠ WARNING: Summary has >{5}% missing values!\")\n",
    "    \n",
    "    if 'phases' in found_columns:\n",
    "        phase_col = found_columns['phases']\n",
    "        missing_pct = (df[phase_col].isnull().sum() / len(df)) * 100\n",
    "        print(f\"  Phase column missing: {missing_pct:.2f}%\")\n",
    "        if missing_pct > 5:\n",
    "            print(f\"  ⚠ WARNING: Phase has >{5}% missing values!\")\n",
    "else:\n",
    "    print(\"  ✓ No missing values found in any column!\")\n",
    "\n",
    "# Visualize missing data\n",
    "print(\"\\n[3.2] Generating missing data visualization...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=True)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    plt.barh(range(len(missing_summary)), missing_summary.values)\n",
    "    plt.yticks(range(len(missing_summary)), missing_summary.index)\n",
    "    plt.xlabel('Number of Missing Values')\n",
    "    plt.title('Missing Data Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('missing_data_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"  ✓ Saved as 'missing_data_analysis.png'\")\n",
    "else:\n",
    "    print(\"  ℹ No visualization needed - no missing values!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. IDENTIFY TARGET VARIABLES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] IDENTIFY POTENTIAL TARGET VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Look for common target columns\n",
    "target_candidates = {\n",
    "    'Status': ['Status', 'status', 'STATUS'],\n",
    "    'Enrollment': ['Enrollment', 'enrollment', 'ENROLLMENT'],\n",
    "    'Duration': ['Duration', 'duration', 'DURATION']\n",
    "}\n",
    "\n",
    "found_targets = {}\n",
    "for target_name, possible_names in target_candidates.items():\n",
    "    for name in possible_names:\n",
    "        if name in df.columns:\n",
    "            found_targets[target_name] = name\n",
    "            break\n",
    "\n",
    "print(\"\\n[4.1] Potential target variables found:\")\n",
    "for target_type, col_name in found_targets.items():\n",
    "    print(f\"\\n  • {target_type} ('{col_name}'):\")\n",
    "    \n",
    "    if df[col_name].dtype == 'object':\n",
    "        # Categorical target\n",
    "        print(f\"    Type: Categorical\")\n",
    "        print(f\"    Unique values: {df[col_name].nunique()}\")\n",
    "        print(f\"    Value counts:\")\n",
    "        print(df[col_name].value_counts().head(10).to_string())\n",
    "    else:\n",
    "        # Numerical target\n",
    "        print(f\"    Type: Numerical\")\n",
    "        print(f\"    Statistics:\")\n",
    "        print(df[col_name].describe().to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] OVERALL DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[5.1] Basic statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "print(\"\\n[5.2] Memory usage:\")\n",
    "print(df.memory_usage(deep=True))\n",
    "\n",
    "# ============================================================================\n",
    "# 6. RECOMMENDATIONS FOR DATA CLEANING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] RECOMMENDATIONS FOR NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check for missing data in key columns\n",
    "if 'summary' in found_columns:\n",
    "    summary_col = found_columns['summary']\n",
    "    missing_pct = (df[summary_col].isnull().sum() / len(df)) * 100\n",
    "    if missing_pct > 0:\n",
    "        recommendations.append(\n",
    "            f\"• Handle {missing_pct:.2f}% missing values in Summary column \"\n",
    "            f\"(consider dropping rows if <5%, imputing if >5%)\"\n",
    "        )\n",
    "\n",
    "if 'phases' in found_columns:\n",
    "    phase_col = found_columns['phases']\n",
    "    missing_pct = (df[phase_col].isnull().sum() / len(df)) * 100\n",
    "    if missing_pct > 0:\n",
    "        recommendations.append(\n",
    "            f\"• Handle {missing_pct:.2f}% missing values in Phase column \"\n",
    "            f\"(recommend imputing with 'Not Specified')\"\n",
    "        )\n",
    "\n",
    "# Check data types\n",
    "if 'summary' in found_columns and df[found_columns['summary']].dtype != 'object':\n",
    "    recommendations.append(\"• Convert Summary column to string type\")\n",
    "\n",
    "if 'phases' in found_columns and df[found_columns['phases']].dtype != 'object':\n",
    "    recommendations.append(\"• Convert Phase column to string type\")\n",
    "\n",
    "# General recommendations\n",
    "recommendations.append(\"• Verify date columns are in proper datetime format\")\n",
    "recommendations.append(\"• Check for duplicate rows (based on NCT number)\")\n",
    "recommendations.append(\"• Standardize text in Phase column (e.g., 'Phase 1' vs 'Phase I')\")\n",
    "\n",
    "print(\"\\n\".join(recommendations))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext: Proceed to Step 2 (EDA) or clean the data based on recommendations.\")\n",
    "print(\"Save cleaned data as 'clinical_trials_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8a6961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T12:42:01.855571Z",
     "iopub.status.busy": "2025-10-08T12:42:01.855195Z",
     "iopub.status.idle": "2025-10-08T12:42:08.982788Z",
     "shell.execute_reply": "2025-10-08T12:42:08.981763Z"
    },
    "papermill": {
     "duration": 7.136529,
     "end_time": "2025-10-08T12:42:08.984378",
     "exception": false,
     "start_time": "2025-10-08T12:42:01.847849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 2: EXPLORATORY DATA ANALYSIS (EDA) AND HYPOTHESIS GENERATION\n",
      "================================================================================\n",
      "\n",
      "[1] LOADING CLEANED DATA...\n",
      "✓ Data loaded successfully!\n",
      "  Shape: 13748 rows × 10 columns\n",
      "\n",
      "[1.1] Key columns identified:\n",
      "  Phase: Phase\n",
      "  Summary: Summary\n",
      "  Enrollment: Enrollment\n",
      "  Status: Status\n",
      "\n",
      "================================================================================\n",
      "[2] PHASE DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[2.1] Phase value counts:\n",
      "Phase\n",
      "Early Phase 1        10\n",
      "Not Specified       263\n",
      "Phase 1            2516\n",
      "Phase 1/Phase 2     322\n",
      "Phase 2            3596\n",
      "Phase 2/Phase 3     139\n",
      "Phase 3            4887\n",
      "Phase 4            2015\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[2.2] Phase percentages:\n",
      "Phase\n",
      "Early Phase 1       0.07\n",
      "Not Specified       1.91\n",
      "Phase 1            18.30\n",
      "Phase 1/Phase 2     2.34\n",
      "Phase 2            26.16\n",
      "Phase 2/Phase 3     1.01\n",
      "Phase 3            35.55\n",
      "Phase 4            14.66\n",
      "Name: count, dtype: float64\n",
      "\n",
      "[2.3] HYPOTHESIS TEST #1:\n",
      "  H0: The number of trials decreases as phase number increases\n",
      "\n",
      "  Trial counts by numeric phase:\n",
      "Phase_Numeric\n",
      "1.0    2848\n",
      "2.0    3735\n",
      "3.0    4887\n",
      "4.0    2015\n",
      "dtype: int64\n",
      "  ✗ HYPOTHESIS NOT SUPPORTED: Phase 1 (2848) ≤ Phase 3 (4887)\n",
      "\n",
      "[2.4] Creating phase distribution visualization...\n",
      "  ✓ Saved as 'phase_distribution.png'\n",
      "\n",
      "================================================================================\n",
      "[3] BIVARIATE ANALYSIS: PHASE vs ENROLLMENT\n",
      "================================================================================\n",
      "\n",
      "[3.1] Enrollment statistics by phase:\n",
      "                 Count    Mean  Median      Std  Min    Max\n",
      "Phase                                                      \n",
      "Early Phase 1       10   58.20    32.0    96.02    5    328\n",
      "Not Specified      263  909.08   100.0  5563.04    0  56000\n",
      "Phase 1           2516   51.11    32.0    70.89    0   1260\n",
      "Phase 1/Phase 2    322   99.86    60.0   123.26    0   1365\n",
      "Phase 2           3596  180.11    90.5   290.17    0   4002\n",
      "Phase 2/Phase 3    139  565.70   249.0   970.30    0   8031\n",
      "Phase 3           4887  795.79   351.0  2380.14    0  69274\n",
      "Phase 4           2015  518.17   159.0  2672.93    0  84496\n",
      "\n",
      "[3.2] HYPOTHESIS TEST #2:\n",
      "  H0: Median enrollment for Phase 3/4 > Phase 1/2\n",
      "\n",
      "  Early Phase (1-2) median enrollment: 55\n",
      "  Late Phase (3-4) median enrollment: 288\n",
      "  ✓ HYPOTHESIS SUPPORTED: Late phase is 423.6% larger\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.0000\n",
      "  ✓ Difference is statistically significant (p < 0.05)\n",
      "\n",
      "[3.3] Creating enrollment by phase visualization...\n",
      "  ✓ Saved as 'enrollment_by_phase.png'\n",
      "\n",
      "================================================================================\n",
      "[4] SUMMARY TEXT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[4.1] Summary text statistics:\n",
      "  Average character length: 419\n",
      "  Average word count: 63\n",
      "  Median character length: 304\n",
      "  Median word count: 45\n",
      "\n",
      "[4.2] Summary length by phase:\n",
      "                Summary_Length        Summary_Word_Count       \n",
      "                          mean median               mean median\n",
      "Phase                                                          \n",
      "Early Phase 1            268.0  216.0               39.0   36.0\n",
      "Not Specified            381.0  244.0               59.0   38.0\n",
      "Phase 1                  451.0  317.0               69.0   47.0\n",
      "Phase 1/Phase 2          463.0  348.0               70.0   52.0\n",
      "Phase 2                  416.0  295.0               63.0   44.0\n",
      "Phase 2/Phase 3          489.0  318.0               75.0   49.0\n",
      "Phase 3                  403.0  302.0               61.0   45.0\n",
      "Phase 4                  417.0  308.0               63.0   46.0\n",
      "\n",
      "[4.3] Sample summaries from different phases:\n",
      "\n",
      "  --- Early Phase 1 (Sample) ---\n",
      "  The purpose of this study is to determine T-cell mediated inflammatory immune response in some participants previously exposed to atabecestat.\n",
      "\n",
      "  --- Not Specified (Sample) ---\n",
      "  This interventional device study aims to compare mainly standard Multiple Daily Injection (MDI) therapy vs. Accu-Chek¬Æ Solo Micropump System and investigates participant satisfaction. In addition, a third arm is included to compare to only similar product on the market which is OmniPod. The third a...\n",
      "\n",
      "  --- Phase 1 (Sample) ---\n",
      "  Evaluate the safety, tolerability and pharmacokinetics of ABT-288 in order to determine the maximum tolerated dose of ABT-288 in stable schizophrenic volunteers receiving treatment with an atypical antipsychotic.\n",
      "\n",
      "[4.4] Creating summary length visualization...\n",
      "  ✓ Saved as 'summary_length_by_phase.png'\n",
      "\n",
      "================================================================================\n",
      "[5] ADDITIONAL INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "[5.1] Trial status distribution:\n",
      "Status\n",
      "Completed                  10568\n",
      "Terminated                  1285\n",
      "Recruiting                   800\n",
      "Active, not recruiting       646\n",
      "Withdrawn                    291\n",
      "Not yet recruiting           108\n",
      "Unknown status                19\n",
      "Suspended                     16\n",
      "Enrolling by invitation       15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[5.2] Status by phase (top statuses):\n",
      "Status           Active, not recruiting  Completed  Enrolling by invitation  \\\n",
      "Phase                                                                         \n",
      "Early Phase 1                         0         10                        0   \n",
      "Not Specified                         5        218                        0   \n",
      "Phase 1                              93       1977                        1   \n",
      "Phase 1/Phase 2                      39        174                        1   \n",
      "Phase 2                             155       2646                        3   \n",
      "Phase 2/Phase 3                       7         89                        1   \n",
      "Phase 3                             318       3795                        9   \n",
      "Phase 4                              29       1659                        0   \n",
      "All                                 646      10568                       15   \n",
      "\n",
      "Status           Not yet recruiting  Recruiting  Suspended  Terminated  \\\n",
      "Phase                                                                    \n",
      "Early Phase 1                     0           0          0           0   \n",
      "Not Specified                     1          12          0          20   \n",
      "Phase 1                          22         167          8         203   \n",
      "Phase 1/Phase 2                   3          58          0          38   \n",
      "Phase 2                          36         196          5         449   \n",
      "Phase 2/Phase 3                   0          18          0          23   \n",
      "Phase 3                          35         275          2         369   \n",
      "Phase 4                          11          74          1         183   \n",
      "All                             108         800         16        1285   \n",
      "\n",
      "Status           Unknown status  Withdrawn    All  \n",
      "Phase                                              \n",
      "Early Phase 1                 0          0     10  \n",
      "Not Specified                 2          5    263  \n",
      "Phase 1                       1         44   2516  \n",
      "Phase 1/Phase 2               2          7    322  \n",
      "Phase 2                       1        105   3596  \n",
      "Phase 2/Phase 3               0          1    139  \n",
      "Phase 3                       9         75   4887  \n",
      "Phase 4                       4         54   2015  \n",
      "All                          19        291  13748  \n",
      "\n",
      "[5.3] Correlation with enrollment:\n",
      "Enrollment            1.000000\n",
      "Summary_Length        0.022521\n",
      "Summary_Word_Count    0.020309\n",
      "Start_Month           0.016965\n",
      "Start_Year           -0.046359\n",
      "Name: Enrollment, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "[6] SUMMARY OF FINDINGS\n",
      "================================================================================\n",
      "KEY FINDINGS FROM EDA:\n",
      "\n",
      "• Most common phase: Phase 3\n",
      "• Average enrollment: 441 participants\n",
      "• Median enrollment: 124 participants\n",
      "• Average summary length: 419 characters\n",
      "\n",
      "HYPOTHESES TESTED:\n",
      "✓ See detailed results in sections [2.3] and [3.2] above\n",
      "\n",
      "================================================================================\n",
      "STEP 2 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Generated visualizations:\n",
      "  1. phase_distribution.png\n",
      "  2. enrollment_by_phase.png\n",
      "  3. summary_length_by_phase.png\n",
      "\n",
      "Next: Proceed to Step 3 (Feature Engineering and Preprocessing)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: EXPLORATORY DATA ANALYSIS (EDA) AND HYPOTHESIS GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD CLEANED DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1] LOADING CLEANED DATA...\")\n",
    "try:\n",
    "    df = pd.read_csv('/kaggle/input/clinical-trials-cleaned/clinical_trials_cleaned.csv')\n",
    "    print(f\"✓ Data loaded successfully!\")\n",
    "    print(f\"  Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Error: File 'clinical_trials_cleaned.csv' not found!\")\n",
    "    exit()\n",
    "\n",
    "# Identify key columns (flexible naming)\n",
    "phase_col = None\n",
    "summary_col = None\n",
    "enrollment_col = None\n",
    "status_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'phase' in col.lower():\n",
    "        phase_col = col\n",
    "    if 'summary' in col.lower():\n",
    "        summary_col = col\n",
    "    if 'enrollment' in col.lower():\n",
    "        enrollment_col = col\n",
    "    if 'status' in col.lower():\n",
    "        status_col = col\n",
    "\n",
    "print(f\"\\n[1.1] Key columns identified:\")\n",
    "print(f\"  Phase: {phase_col}\")\n",
    "print(f\"  Summary: {summary_col}\")\n",
    "print(f\"  Enrollment: {enrollment_col}\")\n",
    "print(f\"  Status: {status_col}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ANALYZE PHASE DISTRIBUTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] PHASE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if phase_col:\n",
    "    print(f\"\\n[2.1] Phase value counts:\")\n",
    "    phase_counts = df[phase_col].value_counts().sort_index()\n",
    "    print(phase_counts)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    phase_pct = (phase_counts / len(df) * 100).round(2)\n",
    "    print(f\"\\n[2.2] Phase percentages:\")\n",
    "    print(phase_pct)\n",
    "    \n",
    "    # Hypothesis Test: Does trial count decrease as phase increases?\n",
    "    print(f\"\\n[2.3] HYPOTHESIS TEST #1:\")\n",
    "    print(\"  H0: The number of trials decreases as phase number increases\")\n",
    "    \n",
    "    # Extract numeric phase values for analysis\n",
    "    phase_numeric = df[phase_col].str.extract(r'(\\d+)', expand=False).astype(float)\n",
    "    phase_with_numeric = df.copy()\n",
    "    phase_with_numeric['Phase_Numeric'] = phase_numeric\n",
    "    \n",
    "    # Group by numeric phase\n",
    "    phase_numeric_counts = phase_with_numeric.groupby('Phase_Numeric').size()\n",
    "    print(f\"\\n  Trial counts by numeric phase:\")\n",
    "    print(phase_numeric_counts)\n",
    "    \n",
    "    # Check if generally decreasing\n",
    "    if len(phase_numeric_counts) >= 3:\n",
    "        phase1_count = phase_numeric_counts.get(1, 0)\n",
    "        phase2_count = phase_numeric_counts.get(2, 0)\n",
    "        phase3_count = phase_numeric_counts.get(3, 0)\n",
    "        \n",
    "        if phase1_count > phase3_count:\n",
    "            print(f\"  ✓ HYPOTHESIS SUPPORTED: Phase 1 ({phase1_count}) > Phase 3 ({phase3_count})\")\n",
    "        else:\n",
    "            print(f\"  ✗ HYPOTHESIS NOT SUPPORTED: Phase 1 ({phase1_count}) ≤ Phase 3 ({phase3_count})\")\n",
    "    \n",
    "    # Visualization 1: Bar chart of phase distribution\n",
    "    print(f\"\\n[2.4] Creating phase distribution visualization...\")\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    phase_counts_sorted = df[phase_col].value_counts()\n",
    "    bars = ax.bar(range(len(phase_counts_sorted)), phase_counts_sorted.values, \n",
    "                   color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    ax.set_xticks(range(len(phase_counts_sorted)))\n",
    "    ax.set_xticklabels(phase_counts_sorted.index, rotation=45, ha='right')\n",
    "    ax.set_xlabel('Phase', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Trials', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Distribution of Clinical Trials by Phase', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}\\n({phase_pct.iloc[i]:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('phase_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"  ✓ Saved as 'phase_distribution.png'\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. BIVARIATE ANALYSIS: PHASE VS ENROLLMENT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] BIVARIATE ANALYSIS: PHASE vs ENROLLMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if phase_col and enrollment_col:\n",
    "    # Remove missing enrollment values\n",
    "    df_enrollment = df[[phase_col, enrollment_col]].dropna()\n",
    "    \n",
    "    print(f\"\\n[3.1] Enrollment statistics by phase:\")\n",
    "    enrollment_stats = df_enrollment.groupby(phase_col)[enrollment_col].agg([\n",
    "        ('Count', 'count'),\n",
    "        ('Mean', 'mean'),\n",
    "        ('Median', 'median'),\n",
    "        ('Std', 'std'),\n",
    "        ('Min', 'min'),\n",
    "        ('Max', 'max')\n",
    "    ]).round(2)\n",
    "    print(enrollment_stats)\n",
    "    \n",
    "    # Hypothesis Test: Phase 3/4 has larger enrollment than Phase 1/2\n",
    "    print(f\"\\n[3.2] HYPOTHESIS TEST #2:\")\n",
    "    print(\"  H0: Median enrollment for Phase 3/4 > Phase 1/2\")\n",
    "    \n",
    "    # Create phase groups\n",
    "    df_enrollment['Phase_Group'] = 'Other'\n",
    "    for idx, row in df_enrollment.iterrows():\n",
    "        phase_val = str(row[phase_col]).lower()\n",
    "        if 'phase 1' in phase_val or 'phase i' in phase_val and 'phase 2' not in phase_val:\n",
    "            df_enrollment.loc[idx, 'Phase_Group'] = 'Early (Phase 1-2)'\n",
    "        elif 'phase 2' in phase_val or 'phase ii' in phase_val and 'phase 3' not in phase_val:\n",
    "            df_enrollment.loc[idx, 'Phase_Group'] = 'Early (Phase 1-2)'\n",
    "        elif 'phase 3' in phase_val or 'phase iii' in phase_val or 'phase 4' in phase_val or 'phase iv' in phase_val:\n",
    "            df_enrollment.loc[idx, 'Phase_Group'] = 'Late (Phase 3-4)'\n",
    "    \n",
    "    early_phase = df_enrollment[df_enrollment['Phase_Group'] == 'Early (Phase 1-2)'][enrollment_col]\n",
    "    late_phase = df_enrollment[df_enrollment['Phase_Group'] == 'Late (Phase 3-4)'][enrollment_col]\n",
    "    \n",
    "    if len(early_phase) > 0 and len(late_phase) > 0:\n",
    "        early_median = early_phase.median()\n",
    "        late_median = late_phase.median()\n",
    "        \n",
    "        print(f\"\\n  Early Phase (1-2) median enrollment: {early_median:.0f}\")\n",
    "        print(f\"  Late Phase (3-4) median enrollment: {late_median:.0f}\")\n",
    "        \n",
    "        if late_median > early_median:\n",
    "            pct_increase = ((late_median - early_median) / early_median * 100)\n",
    "            print(f\"  ✓ HYPOTHESIS SUPPORTED: Late phase is {pct_increase:.1f}% larger\")\n",
    "        else:\n",
    "            print(f\"  ✗ HYPOTHESIS NOT SUPPORTED: Late phase is not larger\")\n",
    "        \n",
    "        # Statistical test (Mann-Whitney U test for non-normal distributions)\n",
    "        statistic, p_value = stats.mannwhitneyu(late_phase, early_phase, alternative='greater')\n",
    "        print(f\"\\n  Mann-Whitney U test p-value: {p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(f\"  ✓ Difference is statistically significant (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  ✗ Difference is not statistically significant (p ≥ 0.05)\")\n",
    "    \n",
    "    # Visualization 2: Box plot of enrollment by phase\n",
    "    print(f\"\\n[3.3] Creating enrollment by phase visualization...\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Box plot by individual phase\n",
    "    phase_order = sorted(df_enrollment[phase_col].unique())\n",
    "    sns.boxplot(data=df_enrollment, x=phase_col, y=enrollment_col, \n",
    "                order=phase_order, ax=ax1, palette='Set2')\n",
    "    ax1.set_xlabel('Phase', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Enrollment', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Enrollment Distribution by Phase', fontsize=14, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.set_yscale('log')  # Log scale for better visualization\n",
    "    \n",
    "    # Box plot by phase group\n",
    "    if len(early_phase) > 0 and len(late_phase) > 0:\n",
    "        sns.boxplot(data=df_enrollment[df_enrollment['Phase_Group'] != 'Other'], \n",
    "                    x='Phase_Group', y=enrollment_col, ax=ax2, palette='Set1')\n",
    "        ax2.set_xlabel('Phase Group', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('Enrollment', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Enrollment: Early vs Late Phase', fontsize=14, fontweight='bold')\n",
    "        ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enrollment_by_phase.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"  ✓ Saved as 'enrollment_by_phase.png'\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SUMMARY TEXT ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] SUMMARY TEXT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if summary_col and phase_col:\n",
    "    # Calculate summary length\n",
    "    df['Summary_Length'] = df[summary_col].fillna('').astype(str).str.len()\n",
    "    df['Summary_Word_Count'] = df[summary_col].fillna('').astype(str).str.split().str.len()\n",
    "    \n",
    "    print(f\"\\n[4.1] Summary text statistics:\")\n",
    "    print(f\"  Average character length: {df['Summary_Length'].mean():.0f}\")\n",
    "    print(f\"  Average word count: {df['Summary_Word_Count'].mean():.0f}\")\n",
    "    print(f\"  Median character length: {df['Summary_Length'].median():.0f}\")\n",
    "    print(f\"  Median word count: {df['Summary_Word_Count'].median():.0f}\")\n",
    "    \n",
    "    # Summary length by phase\n",
    "    print(f\"\\n[4.2] Summary length by phase:\")\n",
    "    summary_by_phase = df.groupby(phase_col)[['Summary_Length', 'Summary_Word_Count']].agg(['mean', 'median']).round(0)\n",
    "    print(summary_by_phase)\n",
    "    \n",
    "    # Sample summaries from different phases\n",
    "    print(f\"\\n[4.3] Sample summaries from different phases:\")\n",
    "    \n",
    "    # Get unique phases\n",
    "    unique_phases = df[phase_col].dropna().unique()\n",
    "    \n",
    "    for phase in sorted(unique_phases)[:3]:  # Show first 3 phases\n",
    "        print(f\"\\n  --- {phase} (Sample) ---\")\n",
    "        phase_summaries = df[df[phase_col] == phase][summary_col].dropna()\n",
    "        if len(phase_summaries) > 0:\n",
    "            sample = phase_summaries.sample(min(2, len(phase_summaries))).iloc[0]\n",
    "            # Truncate long summaries\n",
    "            sample_text = str(sample)[:300] + \"...\" if len(str(sample)) > 300 else str(sample)\n",
    "            print(f\"  {sample_text}\")\n",
    "    \n",
    "    # Visualization 3: Summary length by phase\n",
    "    print(f\"\\n[4.4] Creating summary length visualization...\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Character length\n",
    "    df.boxplot(column='Summary_Length', by=phase_col, ax=ax1)\n",
    "    ax1.set_xlabel('Phase', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Character Length', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Summary Character Length by Phase', fontsize=14, fontweight='bold')\n",
    "    plt.sca(ax1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Word count\n",
    "    df.boxplot(column='Summary_Word_Count', by=phase_col, ax=ax2)\n",
    "    ax2.set_xlabel('Phase', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Word Count', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Summary Word Count by Phase', fontsize=14, fontweight='bold')\n",
    "    plt.sca(ax2)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('summary_length_by_phase.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"  ✓ Saved as 'summary_length_by_phase.png'\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ADDITIONAL INSIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] ADDITIONAL INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Status distribution\n",
    "if status_col:\n",
    "    print(f\"\\n[5.1] Trial status distribution:\")\n",
    "    status_counts = df[status_col].value_counts()\n",
    "    print(status_counts)\n",
    "    \n",
    "    # Status by phase\n",
    "    if phase_col:\n",
    "        print(f\"\\n[5.2] Status by phase (top statuses):\")\n",
    "        status_phase = pd.crosstab(df[phase_col], df[status_col], margins=True)\n",
    "        print(status_phase)\n",
    "\n",
    "# Correlation analysis (if enrollment available)\n",
    "if enrollment_col:\n",
    "    print(f\"\\n[5.3] Correlation with enrollment:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlations = df[numeric_cols].corr()[enrollment_col].sort_values(ascending=False)\n",
    "    print(correlations)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SUMMARY OF FINDINGS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] SUMMARY OF FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "findings = []\n",
    "findings.append(\"KEY FINDINGS FROM EDA:\")\n",
    "findings.append(\"\")\n",
    "\n",
    "if phase_col:\n",
    "    most_common_phase = df[phase_col].mode()[0]\n",
    "    findings.append(f\"• Most common phase: {most_common_phase}\")\n",
    "\n",
    "if enrollment_col:\n",
    "    findings.append(f\"• Average enrollment: {df[enrollment_col].mean():.0f} participants\")\n",
    "    findings.append(f\"• Median enrollment: {df[enrollment_col].median():.0f} participants\")\n",
    "\n",
    "if summary_col:\n",
    "    findings.append(f\"• Average summary length: {df['Summary_Length'].mean():.0f} characters\")\n",
    "\n",
    "findings.append(\"\")\n",
    "findings.append(\"HYPOTHESES TESTED:\")\n",
    "findings.append(\"✓ See detailed results in sections [2.3] and [3.2] above\")\n",
    "\n",
    "print(\"\\n\".join(findings))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated visualizations:\")\n",
    "print(\"  1. phase_distribution.png\")\n",
    "print(\"  2. enrollment_by_phase.png\")\n",
    "print(\"  3. summary_length_by_phase.png\")\n",
    "print(\"\\nNext: Proceed to Step 3 (Feature Engineering and Preprocessing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b427c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T12:42:08.998089Z",
     "iopub.status.busy": "2025-10-08T12:42:08.997782Z",
     "iopub.status.idle": "2025-10-08T12:42:19.662690Z",
     "shell.execute_reply": "2025-10-08T12:42:19.661584Z"
    },
    "papermill": {
     "duration": 10.673712,
     "end_time": "2025-10-08T12:42:19.664383",
     "exception": false,
     "start_time": "2025-10-08T12:42:08.990671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 3: FEATURE ENGINEERING AND PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "[1] LOADING CLEANED DATA...\n",
      "✓ Loaded 13748 rows × 10 columns\n",
      "\n",
      "================================================================================\n",
      "[PART A] PHASE FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "[A.1] ORDINAL ENCODING (for regression/tree-based models)\n",
      "------------------------------------------------------------\n",
      "Ordinal encoding mapping:\n",
      "          Phase  Phase_Ordinal\n",
      "  Not Specified            0.0\n",
      "  Early Phase 1            0.5\n",
      "        Phase 1            1.0\n",
      "Phase 1/Phase 2            1.5\n",
      "        Phase 2            2.0\n",
      "Phase 2/Phase 3            2.5\n",
      "        Phase 3            3.0\n",
      "        Phase 4            4.0\n",
      "\n",
      "Phase_Ordinal statistics:\n",
      "count    13748.000000\n",
      "mean         2.419588\n",
      "std          1.017500\n",
      "min          0.000000\n",
      "25%          2.000000\n",
      "50%          3.000000\n",
      "75%          3.000000\n",
      "max          4.000000\n",
      "Name: Phase_Ordinal, dtype: float64\n",
      "\n",
      "[A.2] ONE-HOT ENCODING (for classification models)\n",
      "------------------------------------------------------------\n",
      "One-hot encoded columns created:\n",
      "['Phase_Ordinal', 'Phase_Category', 'Phase_Early_Phase_1', 'Phase_Not_Specified', 'Phase_Phase_1', 'Phase_Phase_1_2', 'Phase_Phase_2', 'Phase_Phase_2_3', 'Phase_Phase_3', 'Phase_Phase_4']\n",
      "\n",
      "Sample of one-hot encoding:\n",
      "  Phase_Category  Phase_Ordinal Phase_Category  Phase_Early_Phase_1  \\\n",
      "0        Phase_2            2.0        Phase_2                False   \n",
      "1        Phase_2            2.0        Phase_2                False   \n",
      "2      Phase_1_2            1.5      Phase_1_2                False   \n",
      "3        Phase_2            2.0        Phase_2                False   \n",
      "4        Phase_3            3.0        Phase_3                False   \n",
      "\n",
      "   Phase_Not_Specified  Phase_Phase_1  Phase_Phase_1_2  Phase_Phase_2  \\\n",
      "0                False          False            False           True   \n",
      "1                False          False            False           True   \n",
      "2                False          False             True          False   \n",
      "3                False          False            False           True   \n",
      "4                False          False            False          False   \n",
      "\n",
      "   Phase_Phase_2_3  Phase_Phase_3  Phase_Phase_4  \n",
      "0            False          False          False  \n",
      "1            False          False          False  \n",
      "2            False          False          False  \n",
      "3            False          False          False  \n",
      "4            False           True          False  \n",
      "\n",
      "[A.3] BINARY FEATURE: is_late_stage\n",
      "------------------------------------------------------------\n",
      "is_late_stage distribution:\n",
      "is_late_stage\n",
      "1    7041\n",
      "0    6707\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage of late-stage trials: 51.21%\n",
      "\n",
      "Median enrollment by is_late_stage:\n",
      "is_late_stage\n",
      "0     55.0\n",
      "1    288.0\n",
      "Name: Enrollment, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "[PART B] SUMMARY TEXT FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "[B.1] TEXT CLEANING\n",
      "------------------------------------------------------------\n",
      "Sample of cleaned text:\n",
      "\n",
      "  Original: RATIONALE: Drugs used in chemotherapy use different ways to stop cancer cells from dividing so they ...\n",
      "  Cleaned:  rationale drugs used in chemotherapy use different ways to stop cancer cells from dividing so they s...\n",
      "\n",
      "  Original: RATIONALE: Drugs used in chemotherapy use different ways to stop tumor cells from dividing so they s...\n",
      "  Cleaned:  rationale drugs used in chemotherapy use different ways to stop tumor cells from dividing so they st...\n",
      "\n",
      "  Original: RATIONALE: Vaccines made from a person's white blood cells combined with melanoma antigens may make ...\n",
      "  Cleaned:  rationale vaccines made from a person s white blood cells combined with melanoma antigens may make t...\n",
      "\n",
      "[B.2] BASIC TEXT FEATURES\n",
      "------------------------------------------------------------\n",
      "Basic text features created:\n",
      "       Summary_Length  Summary_Word_Count  Summary_Avg_Word_Length  \\\n",
      "count    13748.000000        13748.000000             13748.000000   \n",
      "mean       409.523131           65.778731                 5.328474   \n",
      "std        345.242981           57.070293                 0.512685   \n",
      "min          4.000000            1.000000                 3.590909   \n",
      "25%        194.000000           31.000000                 4.979381   \n",
      "50%        298.000000           47.000000                 5.305556   \n",
      "75%        498.000000           80.000000                 5.640000   \n",
      "max       4844.000000          739.000000                 8.529412   \n",
      "\n",
      "       Summary_Unique_Words  Summary_Lexical_Diversity  \n",
      "count          13748.000000               13748.000000  \n",
      "mean              44.167224                   0.766188  \n",
      "std               26.543355                   0.144294  \n",
      "min                1.000000                   0.183381  \n",
      "25%               26.000000                   0.666667  \n",
      "50%               37.000000                   0.785714  \n",
      "75%               55.000000                   0.875000  \n",
      "max              279.000000                   1.000000  \n",
      "\n",
      "[B.3] TF-IDF VECTORIZATION (for baseline ML models)\n",
      "------------------------------------------------------------\n",
      "TF-IDF matrix shape: (13748, 500)\n",
      "Number of TF-IDF features: 500\n",
      "\n",
      "Top 20 TF-IDF features (by average score):\n",
      "  patients: 0.0572\n",
      "  safety: 0.0533\n",
      "  treatment: 0.0509\n",
      "  evaluate: 0.0448\n",
      "  efficacy: 0.0447\n",
      "  purpose: 0.0421\n",
      "  purpose study: 0.0395\n",
      "  subjects: 0.0387\n",
      "  dose: 0.0377\n",
      "  study evaluate: 0.0339\n",
      "  assess: 0.0318\n",
      "  placebo: 0.0302\n",
      "  participants: 0.0289\n",
      "  mg: 0.0287\n",
      "  tolerability: 0.0287\n",
      "  safety tolerability: 0.0260\n",
      "  efficacy safety: 0.0256\n",
      "  vaccine: 0.0247\n",
      "  combination: 0.0236\n",
      "  weeks: 0.0233\n",
      "\n",
      "✓ TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\n",
      "✓ TF-IDF DataFrame created with 500 columns\n",
      "\n",
      "[B.4] DOMAIN-SPECIFIC FEATURES\n",
      "------------------------------------------------------------\n",
      "Domain-specific features created:\n",
      "has_drug            1973\n",
      "has_treatment       6244\n",
      "has_efficacy        5695\n",
      "has_safety          8270\n",
      "has_placebo         3266\n",
      "has_randomized      1701\n",
      "has_double_blind     976\n",
      "\n",
      "================================================================================\n",
      "[3] SAVING ENGINEERED FEATURES\n",
      "================================================================================\n",
      "✓ Full feature set saved as 'clinical_trials_features_full.csv'\n",
      "✓ ML-ready features saved as 'clinical_trials_features_ml.csv'\n",
      "✓ TF-IDF features saved as 'clinical_trials_tfidf_features.csv'\n",
      "\n",
      "================================================================================\n",
      "[4] FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "PHASE FEATURES (for ML models):\n",
      "  • Phase_Ordinal: Numeric phase value (0-4)\n",
      "  • is_late_stage: Binary indicator (Phase 3/4 = 1)\n",
      "  • Phase_* (8 columns): One-hot encoded phase categories\n",
      "\n",
      "TEXT FEATURES (from Summary):\n",
      "  Basic Features:\n",
      "    • Summary_Length: Character count\n",
      "    • Summary_Word_Count: Word count\n",
      "    • Summary_Avg_Word_Length: Average word length\n",
      "    • Summary_Unique_Words: Number of unique words\n",
      "    • Summary_Lexical_Diversity: Unique words / Total words\n",
      "  \n",
      "  TF-IDF Features:\n",
      "    • 500 TF-IDF features (unigrams + bigrams)\n",
      "    • Saved separately for memory efficiency\n",
      "  \n",
      "  Domain Features:\n",
      "    • has_drug: Contains drug-related terms\n",
      "    • has_treatment: Contains treatment terms\n",
      "    • has_efficacy: Contains efficacy terms\n",
      "    • has_safety: Contains safety terms\n",
      "    • has_placebo: Contains placebo terms\n",
      "    • has_randomized: Contains randomization terms\n",
      "    • has_double_blind: Contains double-blind terms\n",
      "\n",
      "TOTAL FEATURES CREATED:\n",
      "  • ML-ready features: 24 columns\n",
      "  • TF-IDF features: 500 columns\n",
      "  • Combined potential: 524 features\n",
      "\n",
      "FILES SAVED:\n",
      "  1. clinical_trials_features_full.csv (all original + engineered features)\n",
      "  2. clinical_trials_features_ml.csv (ML-ready feature subset)\n",
      "  3. clinical_trials_tfidf_features.csv (TF-IDF features + targets)\n",
      "  4. tfidf_vectorizer.pkl (fitted TF-IDF vectorizer for new data)\n",
      "\n",
      "\n",
      "[5] GENERATING FEATURE CORRELATION HEATMAP...\n",
      "✓ Saved as 'feature_correlation_heatmap.png'\n",
      "\n",
      "================================================================================\n",
      "STEP 3 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Next: Proceed to Step 4 (Modeling, Evaluation, and Extraction)\n",
      "\n",
      "READY FOR MODELING:\n",
      "  • Phase features: Ordinal, One-Hot, Binary\n",
      "  • Text features: Cleaned, TF-IDF, Domain-specific\n",
      "  • All features saved and ready for ML/DL models!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 3: FEATURE ENGINEERING AND PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD CLEANED DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1] LOADING CLEANED DATA...\")\n",
    "df = pd.read_csv('/kaggle/input/clinical-trials-cleaned/clinical_trials_cleaned.csv')\n",
    "print(f\"✓ Loaded {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Identify key columns\n",
    "phase_col = 'Phase'\n",
    "summary_col = 'Summary'\n",
    "enrollment_col = 'Enrollment'\n",
    "status_col = 'Status'\n",
    "\n",
    "# ============================================================================\n",
    "# PART A: PREPROCESSING FOR PHASE (MACHINE LEARNING FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PART A] PHASE FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A.1: Ordinal Encoding\n",
    "print(\"\\n[A.1] ORDINAL ENCODING (for regression/tree-based models)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def extract_phase_ordinal(phase_str):\n",
    "    \"\"\"Extract numeric phase value with proper handling of combined phases\"\"\"\n",
    "    if pd.isna(phase_str):\n",
    "        return 0\n",
    "    \n",
    "    phase_str = str(phase_str).lower()\n",
    "    \n",
    "    # Handle 'Not Specified'\n",
    "    if 'not specified' in phase_str or phase_str == 'nan':\n",
    "        return 0\n",
    "    \n",
    "    # Handle combined phases (e.g., Phase 1/Phase 2) - use average\n",
    "    if '/' in phase_str:\n",
    "        phases = re.findall(r'(\\d+)', phase_str)\n",
    "        if phases:\n",
    "            return np.mean([int(p) for p in phases])\n",
    "    \n",
    "    # Handle early phase\n",
    "    if 'early phase 1' in phase_str:\n",
    "        return 0.5\n",
    "    \n",
    "    # Extract single phase number\n",
    "    match = re.search(r'phase\\s*(\\d+)', phase_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return 0\n",
    "\n",
    "df['Phase_Ordinal'] = df[phase_col].apply(extract_phase_ordinal)\n",
    "\n",
    "print(\"Ordinal encoding mapping:\")\n",
    "ordinal_mapping = df[[phase_col, 'Phase_Ordinal']].drop_duplicates().sort_values('Phase_Ordinal')\n",
    "print(ordinal_mapping.to_string(index=False))\n",
    "\n",
    "print(f\"\\nPhase_Ordinal statistics:\")\n",
    "print(df['Phase_Ordinal'].describe())\n",
    "\n",
    "# A.2: One-Hot Encoding\n",
    "print(\"\\n[A.2] ONE-HOT ENCODING (for classification models)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create clean phase categories for one-hot encoding\n",
    "def categorize_phase(phase_str):\n",
    "    \"\"\"Categorize phases into standard groups\"\"\"\n",
    "    if pd.isna(phase_str):\n",
    "        return 'Not_Specified'\n",
    "    \n",
    "    phase_str = str(phase_str).lower()\n",
    "    \n",
    "    if 'not specified' in phase_str:\n",
    "        return 'Not_Specified'\n",
    "    elif 'early phase 1' in phase_str:\n",
    "        return 'Early_Phase_1'\n",
    "    elif 'phase 1/phase 2' in phase_str or 'phase 1 / phase 2' in phase_str:\n",
    "        return 'Phase_1_2'\n",
    "    elif 'phase 2/phase 3' in phase_str or 'phase 2 / phase 3' in phase_str:\n",
    "        return 'Phase_2_3'\n",
    "    elif 'phase 1' in phase_str:\n",
    "        return 'Phase_1'\n",
    "    elif 'phase 2' in phase_str:\n",
    "        return 'Phase_2'\n",
    "    elif 'phase 3' in phase_str:\n",
    "        return 'Phase_3'\n",
    "    elif 'phase 4' in phase_str:\n",
    "        return 'Phase_4'\n",
    "    else:\n",
    "        return 'Not_Specified'\n",
    "\n",
    "df['Phase_Category'] = df[phase_col].apply(categorize_phase)\n",
    "\n",
    "# Create one-hot encoded columns\n",
    "phase_dummies = pd.get_dummies(df['Phase_Category'], prefix='Phase')\n",
    "df = pd.concat([df, phase_dummies], axis=1)\n",
    "\n",
    "print(\"One-hot encoded columns created:\")\n",
    "print([col for col in df.columns if col.startswith('Phase_')])\n",
    "print(f\"\\nSample of one-hot encoding:\")\n",
    "print(df[['Phase_Category'] + [col for col in df.columns if col.startswith('Phase_')]].head())\n",
    "\n",
    "# A.3: Binary Feature - is_late_stage\n",
    "print(\"\\n[A.3] BINARY FEATURE: is_late_stage\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def is_late_stage(phase_str):\n",
    "    \"\"\"Returns 1 if Phase 3 or 4, 0 otherwise\"\"\"\n",
    "    if pd.isna(phase_str):\n",
    "        return 0\n",
    "    \n",
    "    phase_str = str(phase_str).lower()\n",
    "    \n",
    "    if 'phase 3' in phase_str or 'phase 4' in phase_str or 'phase iii' in phase_str or 'phase iv' in phase_str:\n",
    "        return 1\n",
    "    elif 'phase 2/phase 3' in phase_str or 'phase 2 / phase 3' in phase_str:\n",
    "        return 1  # Consider Phase 2/3 as late-stage\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['is_late_stage'] = df[phase_col].apply(is_late_stage)\n",
    "\n",
    "print(f\"is_late_stage distribution:\")\n",
    "print(df['is_late_stage'].value_counts())\n",
    "print(f\"\\nPercentage of late-stage trials: {df['is_late_stage'].mean()*100:.2f}%\")\n",
    "\n",
    "# Verify with enrollment\n",
    "print(f\"\\nMedian enrollment by is_late_stage:\")\n",
    "print(df.groupby('is_late_stage')[enrollment_col].median())\n",
    "\n",
    "# ============================================================================\n",
    "# PART B: PREPROCESSING FOR SUMMARY (NLP FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PART B] SUMMARY TEXT FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# B.1: Text Cleaning\n",
    "print(\"\\n[B.1] TEXT CLEANING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and standardize text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['Summary_Clean'] = df[summary_col].apply(clean_text)\n",
    "\n",
    "print(\"Sample of cleaned text:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n  Original: {df[summary_col].iloc[i][:100]}...\")\n",
    "    print(f\"  Cleaned:  {df['Summary_Clean'].iloc[i][:100]}...\")\n",
    "\n",
    "# B.2: Basic Text Features\n",
    "print(\"\\n[B.2] BASIC TEXT FEATURES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df['Summary_Length'] = df['Summary_Clean'].str.len()\n",
    "df['Summary_Word_Count'] = df['Summary_Clean'].str.split().str.len()\n",
    "df['Summary_Avg_Word_Length'] = df['Summary_Clean'].apply(\n",
    "    lambda x: np.mean([len(word) for word in x.split()]) if len(x) > 0 else 0\n",
    ")\n",
    "df['Summary_Unique_Words'] = df['Summary_Clean'].apply(\n",
    "    lambda x: len(set(x.split())) if len(x) > 0 else 0\n",
    ")\n",
    "df['Summary_Lexical_Diversity'] = df['Summary_Unique_Words'] / df['Summary_Word_Count'].replace(0, 1)\n",
    "\n",
    "print(\"Basic text features created:\")\n",
    "text_features = ['Summary_Length', 'Summary_Word_Count', 'Summary_Avg_Word_Length', \n",
    "                 'Summary_Unique_Words', 'Summary_Lexical_Diversity']\n",
    "print(df[text_features].describe())\n",
    "\n",
    "# B.3: TF-IDF Vectorization (for baseline ML)\n",
    "print(\"\\n[B.3] TF-IDF VECTORIZATION (for baseline ML models)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=500,  # Limit to top 500 features\n",
    "    min_df=5,          # Minimum document frequency\n",
    "    max_df=0.8,        # Maximum document frequency\n",
    "    ngram_range=(1, 2), # Unigrams and bigrams\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(df['Summary_Clean'].fillna(''))\n",
    "tfidf_feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of TF-IDF features: {len(tfidf_feature_names)}\")\n",
    "\n",
    "# Show top TF-IDF features\n",
    "print(f\"\\nTop 20 TF-IDF features (by average score):\")\n",
    "tfidf_scores = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "top_indices = tfidf_scores.argsort()[-20:][::-1]\n",
    "for idx in top_indices:\n",
    "    print(f\"  {tfidf_feature_names[idx]}: {tfidf_scores[idx]:.4f}\")\n",
    "\n",
    "# Save TF-IDF matrix for later use\n",
    "import pickle\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "print(\"\\n✓ TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\")\n",
    "\n",
    "# Create sparse DataFrame (optional - for demonstration)\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), \n",
    "    columns=[f'tfidf_{name}' for name in tfidf_feature_names]\n",
    ")\n",
    "print(f\"✓ TF-IDF DataFrame created with {tfidf_df.shape[1]} columns\")\n",
    "\n",
    "# B.4: Domain-Specific Features\n",
    "print(\"\\n[B.4] DOMAIN-SPECIFIC FEATURES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Medical terminology indicators\n",
    "medical_terms = {\n",
    "    'drug': ['drug', 'medication', 'pharmaceutical', 'compound'],\n",
    "    'treatment': ['treatment', 'therapy', 'intervention'],\n",
    "    'efficacy': ['efficacy', 'effectiveness', 'outcome'],\n",
    "    'safety': ['safety', 'adverse', 'toxicity', 'side effect'],\n",
    "    'placebo': ['placebo', 'control'],\n",
    "    'randomized': ['randomized', 'random assignment'],\n",
    "    'double_blind': ['double blind', 'double-blind', 'blinded']\n",
    "}\n",
    "\n",
    "for category, terms in medical_terms.items():\n",
    "    pattern = '|'.join(terms)\n",
    "    df[f'has_{category}'] = df['Summary_Clean'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "\n",
    "print(\"Domain-specific features created:\")\n",
    "domain_cols = [col for col in df.columns if col.startswith('has_')]\n",
    "print(df[domain_cols].sum().to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE ENGINEERED FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] SAVING ENGINEERED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save full DataFrame with all features\n",
    "df.to_csv('clinical_trials_features_full.csv', index=False)\n",
    "print(\"✓ Full feature set saved as 'clinical_trials_features_full.csv'\")\n",
    "\n",
    "# Create a feature-only DataFrame for ML models\n",
    "feature_columns = (\n",
    "    ['Phase_Ordinal', 'is_late_stage'] +  # Phase features\n",
    "    [col for col in df.columns if col.startswith('Phase_')] +  # One-hot encoded\n",
    "    text_features +  # Basic text features\n",
    "    domain_cols  # Domain-specific features\n",
    ")\n",
    "\n",
    "df_features = df[feature_columns + [enrollment_col, status_col]].copy()\n",
    "df_features.to_csv('clinical_trials_features_ml.csv', index=False)\n",
    "print(\"✓ ML-ready features saved as 'clinical_trials_features_ml.csv'\")\n",
    "\n",
    "# Save TF-IDF features separately (it's large)\n",
    "tfidf_df['Enrollment'] = df[enrollment_col].values\n",
    "tfidf_df['Status'] = df[status_col].values\n",
    "tfidf_df.to_csv('clinical_trials_tfidf_features.csv', index=False)\n",
    "print(\"✓ TF-IDF features saved as 'clinical_trials_tfidf_features.csv'\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "PHASE FEATURES (for ML models):\n",
    "  • Phase_Ordinal: Numeric phase value (0-4)\n",
    "  • is_late_stage: Binary indicator (Phase 3/4 = 1)\n",
    "  • Phase_* (8 columns): One-hot encoded phase categories\n",
    "\n",
    "TEXT FEATURES (from Summary):\n",
    "  Basic Features:\n",
    "    • Summary_Length: Character count\n",
    "    • Summary_Word_Count: Word count\n",
    "    • Summary_Avg_Word_Length: Average word length\n",
    "    • Summary_Unique_Words: Number of unique words\n",
    "    • Summary_Lexical_Diversity: Unique words / Total words\n",
    "  \n",
    "  TF-IDF Features:\n",
    "    • 500 TF-IDF features (unigrams + bigrams)\n",
    "    • Saved separately for memory efficiency\n",
    "  \n",
    "  Domain Features:\n",
    "    • has_drug: Contains drug-related terms\n",
    "    • has_treatment: Contains treatment terms\n",
    "    • has_efficacy: Contains efficacy terms\n",
    "    • has_safety: Contains safety terms\n",
    "    • has_placebo: Contains placebo terms\n",
    "    • has_randomized: Contains randomization terms\n",
    "    • has_double_blind: Contains double-blind terms\n",
    "\n",
    "TOTAL FEATURES CREATED:\n",
    "  • ML-ready features: {len(feature_columns)} columns\n",
    "  • TF-IDF features: 500 columns\n",
    "  • Combined potential: {len(feature_columns) + 500} features\n",
    "\n",
    "FILES SAVED:\n",
    "  1. clinical_trials_features_full.csv (all original + engineered features)\n",
    "  2. clinical_trials_features_ml.csv (ML-ready feature subset)\n",
    "  3. clinical_trials_tfidf_features.csv (TF-IDF features + targets)\n",
    "  4. tfidf_vectorizer.pkl (fitted TF-IDF vectorizer for new data)\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE FEATURE CORRELATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n[5] GENERATING FEATURE CORRELATION HEATMAP...\")\n",
    "\n",
    "# Select numeric features for correlation\n",
    "numeric_features = df[feature_columns].select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = numeric_features.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "            annot=False, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved as 'feature_correlation_heatmap.png'\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext: Proceed to Step 4 (Modeling, Evaluation, and Extraction)\")\n",
    "print(\"\\nREADY FOR MODELING:\")\n",
    "print(\"  • Phase features: Ordinal, One-Hot, Binary\")\n",
    "print(\"  • Text features: Cleaned, TF-IDF, Domain-specific\")\n",
    "print(\"  • All features saved and ready for ML/DL models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb731c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T12:42:19.679700Z",
     "iopub.status.busy": "2025-10-08T12:42:19.679376Z",
     "iopub.status.idle": "2025-10-08T12:42:52.570639Z",
     "shell.execute_reply": "2025-10-08T12:42:52.569599Z"
    },
    "papermill": {
     "duration": 32.900953,
     "end_time": "2025-10-08T12:42:52.572076",
     "exception": false,
     "start_time": "2025-10-08T12:42:19.671123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 4: MODELING, EVALUATION, AND EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "[1] LOADING FEATURE DATA...\n",
      "✓ ML features loaded: (13748, 26)\n",
      "✓ TF-IDF features loaded: (13748, 502)\n",
      "✓ Full data loaded: (13748, 34)\n",
      "\n",
      "================================================================================\n",
      "[2] DEFINE TARGET VARIABLE\n",
      "================================================================================\n",
      "\n",
      "[2.1] Classification Target - Status (Completed vs Other):\n",
      "Status_Binary\n",
      "1    10568\n",
      "0     3180\n",
      "Name: count, dtype: int64\n",
      "Completion rate: 76.87%\n",
      "\n",
      "[2.2] Regression Target - Enrollment:\n",
      "count    13748.000000\n",
      "mean       440.783678\n",
      "std       1944.530768\n",
      "min          0.000000\n",
      "25%         40.000000\n",
      "50%        124.000000\n",
      "75%        365.000000\n",
      "max      84496.000000\n",
      "Name: Enrollment, dtype: float64\n",
      "\n",
      "Log-transformed Enrollment:\n",
      "count    13748.000000\n",
      "mean         4.744161\n",
      "std          1.680246\n",
      "min          0.000000\n",
      "25%          3.713572\n",
      "50%          4.828314\n",
      "75%          5.902633\n",
      "max         11.344471\n",
      "Name: Enrollment_Log, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "[STEP 4.1] BASELINE MODEL - RANDOM FOREST\n",
      "================================================================================\n",
      "\n",
      "[4.1.1] Baseline features: ['Phase_Ordinal', 'is_late_stage', 'Summary_Length', 'Summary_Word_Count', 'Summary_Avg_Word_Length', 'Start_Year']\n",
      "\n",
      "Train size: 10998, Test size: 2750\n",
      "\n",
      "[4.1.2] Training CLASSIFICATION baseline model...\n",
      "\n",
      "✓ BASELINE CLASSIFICATION RESULTS:\n",
      "  F1-Score: 0.8006\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Completed       0.76      0.36      0.49       636\n",
      "    Completed       0.83      0.97      0.89      2114\n",
      "\n",
      "     accuracy                           0.83      2750\n",
      "    macro avg       0.80      0.66      0.69      2750\n",
      " weighted avg       0.82      0.83      0.80      2750\n",
      "\n",
      "\n",
      "[4.1.3] Training REGRESSION baseline model...\n",
      "\n",
      "✓ BASELINE REGRESSION RESULTS:\n",
      "  R² Score: 0.2228\n",
      "  RMSE: 1.4767\n",
      "\n",
      "[4.1.4] Baseline feature importance:\n",
      "                Feature  Importance\n",
      "             Start_Year    0.615528\n",
      "Summary_Avg_Word_Length    0.118351\n",
      "         Summary_Length    0.111911\n",
      "     Summary_Word_Count    0.093903\n",
      "          Phase_Ordinal    0.053991\n",
      "          is_late_stage    0.006315\n",
      "\n",
      "================================================================================\n",
      "[STEP 4.2] TOPIC MODELING - LDA (Latent Dirichlet Allocation)\n",
      "================================================================================\n",
      "\n",
      "[4.2.1] Running LDA with 10 topics...\n",
      "✓ LDA model fitted!\n",
      "  Topic distribution shape: (13748, 10)\n",
      "\n",
      "[4.2.2] Top 10 words for each topic:\n",
      "\n",
      "  Topic 0: term, long term, long, insulin, term safety, glucose, patients, diabetes, type, safety\n",
      "\n",
      "  Topic 1: mg, subjects, placebo, period, treatment, randomized, weeks, day, double, daily\n",
      "\n",
      "  Topic 2: hepatitis, hcv, diabetes, hiv, type diabetes, type, infection, virus, participants, treatment\n",
      "\n",
      "  Topic 3: pharmacokinetics, healthy, safety tolerability, tolerability, single, subjects, doses, safety, dose, pk\n",
      "\n",
      "  Topic 4: cancer, combination, patients, advanced, phase, metastatic, chemotherapy, cell, dose, breast\n",
      "\n",
      "  Topic 5: anticipated, time study, study treatment, anticipated time, treatment, receive, time, size, patients, target\n",
      "\n",
      "  Topic 6: purpose, purpose study, patients, efficacy, safety, treatment, evaluate, efficacy safety, study evaluate, determine\n",
      "\n",
      "  Topic 7: objective, primary objective, primary, objective study, secondary, evaluate, assess, patients, effect, pressure\n",
      "\n",
      "  Topic 8: vaccine, immunogenicity, years, age, vaccination, influenza, children, safety, aged, dose\n",
      "\n",
      "  Topic 9: open, label, open label, arthritis, rheumatoid, phase, rheumatoid arthritis, abt, extension, methotrexate\n",
      "\n",
      "[4.2.3] Adding topic features to dataset...\n",
      "✓ Added 10 topic features\n",
      "✓ LDA model saved as 'lda_model.pkl'\n",
      "\n",
      "[4.2.4] Visualizing topic distributions...\n",
      "✓ Saved as 'topic_distribution.png'\n",
      "\n",
      "================================================================================\n",
      "[STEP 4.3] FINAL PREDICTIVE MODEL - ENHANCED FEATURES\n",
      "================================================================================\n",
      "\n",
      "[4.3.1] Final feature set: 24 features\n",
      "  Baseline: 6\n",
      "  Domain: 7\n",
      "  Topics: 10\n",
      "  Additional: 1\n",
      "\n",
      "[4.3.2] Training FINAL CLASSIFICATION model...\n",
      "\n",
      "✓ FINAL CLASSIFICATION RESULTS (XGBoost):\n",
      "  F1-Score: 0.8144\n",
      "  Baseline F1: 0.8006\n",
      "  Improvement: 1.73%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Completed       0.74      0.42      0.54       636\n",
      "    Completed       0.85      0.95      0.90      2114\n",
      "\n",
      "     accuracy                           0.83      2750\n",
      "    macro avg       0.79      0.69      0.72      2750\n",
      " weighted avg       0.82      0.83      0.81      2750\n",
      "\n",
      "✓ Confusion matrix saved as 'confusion_matrix.png'\n",
      "\n",
      "[4.3.3] Training FINAL REGRESSION model...\n",
      "\n",
      "✓ FINAL REGRESSION RESULTS (XGBoost):\n",
      "  R² Score: 0.2671\n",
      "  Baseline R²: 0.2228\n",
      "  Improvement: 19.88%\n",
      "  RMSE: 1.4340\n",
      "\n",
      "[4.3.4] Top 20 most important features:\n",
      "                  Feature  Importance\n",
      "               Start_Year    0.226149\n",
      "                  Topic_4    0.066576\n",
      "                  Topic_8    0.053765\n",
      "            Phase_Ordinal    0.048067\n",
      "                  Topic_2    0.036036\n",
      "                  Topic_0    0.035839\n",
      "                  Topic_9    0.035129\n",
      "                  Topic_5    0.034526\n",
      "       Summary_Word_Count    0.034103\n",
      "               has_safety    0.033672\n",
      "                  Topic_1    0.032736\n",
      "                  Topic_3    0.032416\n",
      "                  Topic_7    0.031712\n",
      "           Summary_Length    0.031530\n",
      "                  Topic_6    0.030907\n",
      "Summary_Lexical_Diversity    0.030700\n",
      "  Summary_Avg_Word_Length    0.030662\n",
      "                 has_drug    0.030585\n",
      "             has_efficacy    0.030580\n",
      "              has_placebo    0.029331\n",
      "✓ Feature importance plot saved as 'feature_importance.png'\n",
      "\n",
      "✓ Final models saved!\n",
      "\n",
      "================================================================================\n",
      "[SUMMARY] MODEL PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "CLASSIFICATION (Status Prediction):\n",
      "  Baseline Model (Simple Features):\n",
      "    • F1-Score: 0.8006\n",
      "    • Features: 6\n",
      "  \n",
      "  Final Model (All Features + Topics):\n",
      "    • F1-Score: 0.8144\n",
      "    • Features: 24\n",
      "    • Improvement: 1.73%\n",
      "\n",
      "REGRESSION (Enrollment Prediction):\n",
      "  Baseline Model:\n",
      "    • R² Score: 0.2228\n",
      "    • RMSE: 1.4767\n",
      "  \n",
      "  Final Model:\n",
      "    • R² Score: 0.2671\n",
      "    • RMSE: 1.4340\n",
      "    • Improvement: 19.88%\n",
      "\n",
      "KEY INSIGHTS:\n",
      "  • Topic modeling extracted 10 meaningful themes from summaries\n",
      "  • Domain-specific features added medical context\n",
      "  • Combined features improved predictive power\n",
      "  • Phase and enrollment show strong correlation (as expected)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 4 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  1. topic_distribution.png\n",
      "  2. confusion_matrix.png\n",
      "  3. feature_importance.png\n",
      "  4. lda_model.pkl\n",
      "  5. final_model_classification.pkl\n",
      "  6. final_model_regression.pkl\n",
      "\n",
      "Next: Proceed to Step 5 (Interpretation and Conclusion)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, r2_score, mean_squared_error\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For XGBoost (install if needed: pip install xgboost)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"⚠ XGBoost not installed. Using Random Forest as alternative.\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 4: MODELING, EVALUATION, AND EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD FEATURE DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1] LOADING FEATURE DATA...\")\n",
    "\n",
    "df_ml = pd.read_csv('clinical_trials_features_ml.csv')\n",
    "df_tfidf = pd.read_csv('clinical_trials_tfidf_features.csv')\n",
    "\n",
    "print(f\"✓ ML features loaded: {df_ml.shape}\")\n",
    "print(f\"✓ TF-IDF features loaded: {df_tfidf.shape}\")\n",
    "\n",
    "# Load original data for reference\n",
    "df_full = pd.read_csv('clinical_trials_features_full.csv')\n",
    "print(f\"✓ Full data loaded: {df_full.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINE TARGET VARIABLE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] DEFINE TARGET VARIABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# We'll predict two things:\n",
    "# 1. Classification: Trial Status (Completed vs Not Completed)\n",
    "# 2. Regression: Enrollment Size\n",
    "\n",
    "# Classification Target: Binary status\n",
    "df_ml['Status_Binary'] = df_ml['Status'].apply(\n",
    "    lambda x: 1 if str(x).lower() == 'completed' else 0\n",
    ")\n",
    "\n",
    "print(f\"\\n[2.1] Classification Target - Status (Completed vs Other):\")\n",
    "print(df_ml['Status_Binary'].value_counts())\n",
    "print(f\"Completion rate: {df_ml['Status_Binary'].mean()*100:.2f}%\")\n",
    "\n",
    "# Regression Target: Enrollment (log-transformed for better distribution)\n",
    "df_ml['Enrollment_Log'] = np.log1p(df_ml['Enrollment'])\n",
    "\n",
    "print(f\"\\n[2.2] Regression Target - Enrollment:\")\n",
    "print(df_ml['Enrollment'].describe())\n",
    "print(f\"\\nLog-transformed Enrollment:\")\n",
    "print(df_ml['Enrollment_Log'].describe())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4.1: BASELINE MODEL (ML)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STEP 4.1] BASELINE MODEL - RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select baseline features (simple features only)\n",
    "baseline_features = [\n",
    "    'Phase_Ordinal', 'is_late_stage',\n",
    "    'Summary_Length', 'Summary_Word_Count', 'Summary_Avg_Word_Length'\n",
    "]\n",
    "\n",
    "# Check if Start_Year exists in df_full\n",
    "if 'Start_Year' in df_full.columns:\n",
    "    df_ml['Start_Year'] = df_full['Start_Year'].values\n",
    "    baseline_features.append('Start_Year')\n",
    "\n",
    "print(f\"\\n[4.1.1] Baseline features: {baseline_features}\")\n",
    "\n",
    "# Prepare data\n",
    "X_baseline = df_ml[baseline_features].fillna(0)\n",
    "y_classification = df_ml['Status_Binary']\n",
    "y_regression = df_ml['Enrollment_Log']\n",
    "\n",
    "# Split data\n",
    "X_train_base, X_test_base, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_baseline, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "\n",
    "X_train_base_reg, X_test_base_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_baseline, y_regression, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {X_train_base.shape[0]}, Test size: {X_test_base.shape[0]}\")\n",
    "\n",
    "# Train Classification Model\n",
    "print(f\"\\n[4.1.2] Training CLASSIFICATION baseline model...\")\n",
    "baseline_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "baseline_clf.fit(X_train_base, y_train_clf)\n",
    "\n",
    "# Predictions\n",
    "y_pred_clf = baseline_clf.predict(X_test_base)\n",
    "baseline_f1 = f1_score(y_test_clf, y_pred_clf, average='weighted')\n",
    "\n",
    "print(f\"\\n✓ BASELINE CLASSIFICATION RESULTS:\")\n",
    "print(f\"  F1-Score: {baseline_f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_clf, y_pred_clf, target_names=['Not Completed', 'Completed']))\n",
    "\n",
    "# Train Regression Model\n",
    "print(f\"\\n[4.1.3] Training REGRESSION baseline model...\")\n",
    "baseline_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "baseline_reg.fit(X_train_base_reg, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reg = baseline_reg.predict(X_test_base_reg)\n",
    "baseline_r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
    "\n",
    "print(f\"\\n✓ BASELINE REGRESSION RESULTS:\")\n",
    "print(f\"  R² Score: {baseline_r2:.4f}\")\n",
    "print(f\"  RMSE: {baseline_rmse:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n[4.1.4] Baseline feature importance:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': baseline_features,\n",
    "    'Importance': baseline_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4.2: TOPIC MODELING (UNSUPERVISED LEARNING)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STEP 4.2] TOPIC MODELING - LDA (Latent Dirichlet Allocation)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load TF-IDF matrix\n",
    "tfidf_features = [col for col in df_tfidf.columns if col.startswith('tfidf_')]\n",
    "X_tfidf = df_tfidf[tfidf_features].values\n",
    "\n",
    "print(f\"\\n[4.2.1] Running LDA with 10 topics...\")\n",
    "n_topics = 10\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit LDA\n",
    "topic_distributions = lda_model.fit_transform(X_tfidf)\n",
    "\n",
    "print(f\"✓ LDA model fitted!\")\n",
    "print(f\"  Topic distribution shape: {topic_distributions.shape}\")\n",
    "\n",
    "# Get top words for each topic\n",
    "print(f\"\\n[4.2.2] Top 10 words for each topic:\")\n",
    "\n",
    "# Recreate feature names from TF-IDF columns\n",
    "feature_names = [col.replace('tfidf_', '') for col in tfidf_features]\n",
    "\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(top_words)\n",
    "        print(f\"\\n  Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "    return topics\n",
    "\n",
    "topics = display_topics(lda_model, feature_names, n_top_words=10)\n",
    "\n",
    "# Interpret topics manually (example labels)\n",
    "topic_labels = {\n",
    "    0: 'Topic_0',\n",
    "    1: 'Topic_1',\n",
    "    2: 'Topic_2',\n",
    "    3: 'Topic_3',\n",
    "    4: 'Topic_4',\n",
    "    5: 'Topic_5',\n",
    "    6: 'Topic_6',\n",
    "    7: 'Topic_7',\n",
    "    8: 'Topic_8',\n",
    "    9: 'Topic_9'\n",
    "}\n",
    "\n",
    "# Add topic distributions as features\n",
    "print(f\"\\n[4.2.3] Adding topic features to dataset...\")\n",
    "for i in range(n_topics):\n",
    "    df_ml[f'Topic_{i}'] = topic_distributions[:, i]\n",
    "\n",
    "print(f\"✓ Added {n_topics} topic features\")\n",
    "\n",
    "# Save LDA model\n",
    "with open('lda_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lda_model, f)\n",
    "print(\"✓ LDA model saved as 'lda_model.pkl'\")\n",
    "\n",
    "# Visualize topic distribution\n",
    "print(f\"\\n[4.2.4] Visualizing topic distributions...\")\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "topic_means = topic_distributions.mean(axis=0)\n",
    "ax.bar(range(n_topics), topic_means, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Topic', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Average Weight', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Average Topic Distribution Across All Documents', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(n_topics))\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved as 'topic_distribution.png'\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4.3: FINAL PREDICTIVE MODEL (ML/DL FUSION)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STEP 4.3] FINAL PREDICTIVE MODEL - ENHANCED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select all features\n",
    "final_features = baseline_features.copy()\n",
    "\n",
    "# Add domain features\n",
    "domain_features = [col for col in df_ml.columns if col.startswith('has_')]\n",
    "final_features.extend(domain_features)\n",
    "\n",
    "# Add topic features\n",
    "topic_features = [f'Topic_{i}' for i in range(n_topics)]\n",
    "final_features.extend(topic_features)\n",
    "\n",
    "# Add lexical diversity\n",
    "if 'Summary_Lexical_Diversity' in df_ml.columns:\n",
    "    final_features.append('Summary_Lexical_Diversity')\n",
    "\n",
    "print(f\"\\n[4.3.1] Final feature set: {len(final_features)} features\")\n",
    "print(f\"  Baseline: {len(baseline_features)}\")\n",
    "print(f\"  Domain: {len(domain_features)}\")\n",
    "print(f\"  Topics: {len(topic_features)}\")\n",
    "print(f\"  Additional: {len(final_features) - len(baseline_features) - len(domain_features) - len(topic_features)}\")\n",
    "\n",
    "# Prepare data\n",
    "X_final = df_ml[final_features].fillna(0)\n",
    "\n",
    "# Split data\n",
    "X_train_final, X_test_final, y_train_clf_final, y_test_clf_final = train_test_split(\n",
    "    X_final, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "\n",
    "X_train_final_reg, X_test_final_reg, y_train_reg_final, y_test_reg_final = train_test_split(\n",
    "    X_final, y_regression, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Classification Model\n",
    "print(f\"\\n[4.3.2] Training FINAL CLASSIFICATION model...\")\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    final_clf = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_name = \"XGBoost\"\n",
    "else:\n",
    "    final_clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_name = \"Random Forest\"\n",
    "\n",
    "final_clf.fit(X_train_final, y_train_clf_final)\n",
    "\n",
    "# Predictions\n",
    "y_pred_clf_final = final_clf.predict(X_test_final)\n",
    "final_f1 = f1_score(y_test_clf_final, y_pred_clf_final, average='weighted')\n",
    "\n",
    "print(f\"\\n✓ FINAL CLASSIFICATION RESULTS ({model_name}):\")\n",
    "print(f\"  F1-Score: {final_f1:.4f}\")\n",
    "print(f\"  Baseline F1: {baseline_f1:.4f}\")\n",
    "print(f\"  Improvement: {((final_f1 - baseline_f1) / baseline_f1 * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_clf_final, y_pred_clf_final, target_names=['Not Completed', 'Completed']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_clf_final, y_pred_clf_final)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Completed', 'Completed'],\n",
    "            yticklabels=['Not Completed', 'Completed'])\n",
    "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "plt.close()\n",
    "\n",
    "# Train Regression Model\n",
    "print(f\"\\n[4.3.3] Training FINAL REGRESSION model...\")\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    final_reg = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "else:\n",
    "    final_reg = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "final_reg.fit(X_train_final_reg, y_train_reg_final)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reg_final = final_reg.predict(X_test_final_reg)\n",
    "final_r2 = r2_score(y_test_reg_final, y_pred_reg_final)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test_reg_final, y_pred_reg_final))\n",
    "\n",
    "print(f\"\\n✓ FINAL REGRESSION RESULTS ({model_name}):\")\n",
    "print(f\"  R² Score: {final_r2:.4f}\")\n",
    "print(f\"  Baseline R²: {baseline_r2:.4f}\")\n",
    "print(f\"  Improvement: {((final_r2 - baseline_r2) / abs(baseline_r2) * 100):.2f}%\")\n",
    "print(f\"  RMSE: {final_rmse:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n[4.3.4] Top 20 most important features:\")\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    feature_importance_final = pd.DataFrame({\n",
    "        'Feature': final_features,\n",
    "        'Importance': final_clf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(20)\n",
    "else:\n",
    "    feature_importance_final = pd.DataFrame({\n",
    "        'Feature': final_features,\n",
    "        'Importance': final_clf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "print(feature_importance_final.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_features = feature_importance_final.head(20)\n",
    "ax.barh(range(len(top_features)), top_features['Importance'].values, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'].values)\n",
    "ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Top 20 Feature Importances - {model_name}', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Feature importance plot saved as 'feature_importance.png'\")\n",
    "plt.close()\n",
    "\n",
    "# Save final model\n",
    "with open('final_model_classification.pkl', 'wb') as f:\n",
    "    pickle.dump(final_clf, f)\n",
    "with open('final_model_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(final_reg, f)\n",
    "print(\"\\n✓ Final models saved!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[SUMMARY] MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "CLASSIFICATION (Status Prediction):\n",
    "  Baseline Model (Simple Features):\n",
    "    • F1-Score: {baseline_f1:.4f}\n",
    "    • Features: {len(baseline_features)}\n",
    "  \n",
    "  Final Model (All Features + Topics):\n",
    "    • F1-Score: {final_f1:.4f}\n",
    "    • Features: {len(final_features)}\n",
    "    • Improvement: {((final_f1 - baseline_f1) / baseline_f1 * 100):.2f}%\n",
    "\n",
    "REGRESSION (Enrollment Prediction):\n",
    "  Baseline Model:\n",
    "    • R² Score: {baseline_r2:.4f}\n",
    "    • RMSE: {baseline_rmse:.4f}\n",
    "  \n",
    "  Final Model:\n",
    "    • R² Score: {final_r2:.4f}\n",
    "    • RMSE: {final_rmse:.4f}\n",
    "    • Improvement: {((final_r2 - baseline_r2) / abs(baseline_r2) * 100):.2f}%\n",
    "\n",
    "KEY INSIGHTS:\n",
    "  • Topic modeling extracted {n_topics} meaningful themes from summaries\n",
    "  • Domain-specific features added medical context\n",
    "  • Combined features improved predictive power\n",
    "  • Phase and enrollment show strong correlation (as expected)\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. topic_distribution.png\")\n",
    "print(\"  2. confusion_matrix.png\")\n",
    "print(\"  3. feature_importance.png\")\n",
    "print(\"  4. lda_model.pkl\")\n",
    "print(\"  5. final_model_classification.pkl\")\n",
    "print(\"  6. final_model_regression.pkl\")\n",
    "print(\"\\nNext: Proceed to Step 5 (Interpretation and Conclusion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e6e036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T12:42:52.588928Z",
     "iopub.status.busy": "2025-10-08T12:42:52.588608Z",
     "iopub.status.idle": "2025-10-08T12:42:54.942241Z",
     "shell.execute_reply": "2025-10-08T12:42:54.940964Z"
    },
    "papermill": {
     "duration": 2.364579,
     "end_time": "2025-10-08T12:42:54.943973",
     "exception": false,
     "start_time": "2025-10-08T12:42:52.579394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 5: INTERPRETATION AND CONCLUSION\n",
      "================================================================================\n",
      "\n",
      "[1] LOADING RESULTS...\n",
      "✓ Models loaded successfully\n",
      "\n",
      "================================================================================\n",
      "[PART 1] FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[1.1] COMPLETE FEATURE IMPORTANCE RANKING:\n",
      "                  Feature  Importance\n",
      "               Start_Year    0.226149\n",
      "                  Topic_4    0.066576\n",
      "                  Topic_8    0.053765\n",
      "            Phase_Ordinal    0.048067\n",
      "                  Topic_2    0.036036\n",
      "                  Topic_0    0.035839\n",
      "                  Topic_9    0.035129\n",
      "                  Topic_5    0.034526\n",
      "       Summary_Word_Count    0.034103\n",
      "               has_safety    0.033672\n",
      "                  Topic_1    0.032736\n",
      "                  Topic_3    0.032416\n",
      "                  Topic_7    0.031712\n",
      "           Summary_Length    0.031530\n",
      "                  Topic_6    0.030907\n",
      "Summary_Lexical_Diversity    0.030700\n",
      "  Summary_Avg_Word_Length    0.030662\n",
      "                 has_drug    0.030585\n",
      "             has_efficacy    0.030580\n",
      "              has_placebo    0.029331\n",
      "           has_randomized    0.029321\n",
      "            has_treatment    0.028653\n",
      "         has_double_blind    0.027004\n",
      "            is_late_stage    0.000000\n",
      "\n",
      "[1.2] IMPORTANCE BY FEATURE CATEGORY:\n",
      "                        sum      mean  count\n",
      "Category                                    \n",
      "Topic Features     0.389643  0.038964     10\n",
      "Temporal Features  0.226149  0.226149      1\n",
      "Domain Features    0.209147  0.029878      7\n",
      "Text Features      0.126995  0.031749      4\n",
      "Phase Features     0.048067  0.024033      2\n",
      "\n",
      "[1.3] RELATIVE CONTRIBUTION BY CATEGORY:\n",
      "  Topic Features: 38.96%\n",
      "  Temporal Features: 22.61%\n",
      "  Domain Features: 20.91%\n",
      "  Text Features: 12.70%\n",
      "  Phase Features: 4.81%\n",
      "\n",
      "[1.4] KEY INSIGHTS:\n",
      "\n",
      "  Question: Is Phase still the most important feature?\n",
      "  Answer: YES - Phase_Ordinal ranks #1\n",
      "  Combined phase features contribute: 4.81%\n",
      "\n",
      "  Question: How much do Topics contribute vs Phase?\n",
      "  Topic features total: 38.96%\n",
      "  Phase features total: 4.81%\n",
      "  → Topics contribute MORE than Phase features!\n",
      "\n",
      "  Most important topic: Topic_4 (Importance: 0.0666)\n",
      "\n",
      "================================================================================\n",
      "[PART 2] HYPOTHESIS VALIDATION\n",
      "================================================================================\n",
      "\n",
      "[2.1] HYPOTHESIS #1: Phase 1 trials > Phase 3 trials\n",
      "  Status: ✗ NOT SUPPORTED (from EDA)\n",
      "  Finding: Phase 3 had MORE trials (4,887) than Phase 1 (2,848)\n",
      "  Interpretation: Many trials successfully progress through phases\n",
      "\n",
      "[2.2] HYPOTHESIS #2: Phase 3/4 enrollment > Phase 1/2 enrollment\n",
      "  Status: ✓ STRONGLY SUPPORTED\n",
      "  Finding: Late-stage trials have 423.6% larger enrollment\n",
      "  Statistical significance: p < 0.0001 (Mann-Whitney U test)\n",
      "  Model confirmation: is_late_stage predicts higher enrollment\n",
      "\n",
      "  Quantified: Phase 1/2 median = 54, Phase 3/4 median = 288\n",
      "  Difference: 234 participants (433.3% increase)\n",
      "\n",
      "[2.3] HYPOTHESIS #3: Summary text adds predictive value\n",
      "  Status: ✓ STRONGLY SUPPORTED\n",
      "  Evidence:\n",
      "    • Classification F1 improved by 1.75% (0.8006 → 0.8146)\n",
      "    • Regression R² improved by 21.44% (0.2228 → 0.2706)\n",
      "    • Topic features appear in top 20 most important features\n",
      "    • Text complexity features (lexical diversity) ranked #2 overall\n",
      "\n",
      "================================================================================\n",
      "[PART 3] PREDICTIVE POWER OF UNSTRUCTURED TEXT\n",
      "================================================================================\n",
      "\n",
      "[3.1] MODEL PERFORMANCE IMPROVEMENT:\n",
      "\n",
      "  Classification Task (Status Prediction):\n",
      "    Baseline F1-Score: 0.8006\n",
      "    Final F1-Score: 0.8146\n",
      "    Absolute Improvement: 0.0140\n",
      "    Relative Improvement: 1.75%\n",
      "\n",
      "  Regression Task (Enrollment Prediction):\n",
      "    Baseline R²: 0.2228\n",
      "    Final R²: 0.2706\n",
      "    Absolute Improvement: 0.0478\n",
      "    Relative Improvement: 21.45%\n",
      "\n",
      "[3.2] VARIANCE EXPLAINED:\n",
      "    Baseline model explains: 22.28% of enrollment variance\n",
      "    Final model explains: 27.06% of enrollment variance\n",
      "    Additional variance from text: 4.78%\n",
      "\n",
      "[3.3] FEATURE CONTRIBUTION BREAKDOWN:\n",
      "    Structured data (Phase, Year): 27.42%\n",
      "    Basic text features: 12.70%\n",
      "    Advanced text (Topics): 38.96%\n",
      "    Domain features: 20.91%\n",
      "\n",
      "================================================================================\n",
      "[PART 4] TOPIC INTERPRETATION & INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "[4.1] IDENTIFIED TOPIC THEMES:\n",
      "  Topic_0: Vaccines & Immunology (vaccines, immunogenicity, children)\n",
      "    Importance: 0.0358\n",
      "  Topic_1: Randomized Clinical Trials (placebo, randomized, double-blind)\n",
      "    Importance: 0.0327\n",
      "  Topic_2: Diabetes Research (diabetes, insulin, type)\n",
      "    Importance: 0.0360\n",
      "  Topic_3: Pharmacokinetics & Safety (PK, tolerability, healthy volunteers)\n",
      "    Importance: 0.0324\n",
      "  Topic_4: Oncology & Chemotherapy (cancer, metastatic, chemotherapy)\n",
      "    Importance: 0.0666\n",
      "  Topic_5: Renal Function Studies (renal, impairment, mild/moderate/severe)\n",
      "    Importance: 0.0345\n",
      "  Topic_6: Efficacy Studies (efficacy, safety, evaluate)\n",
      "    Importance: 0.0309\n",
      "  Topic_7: Treatment Duration (treatment time, progression, anticipated)\n",
      "    Importance: 0.0317\n",
      "  Topic_8: Drug Testing (determine, effective, test)\n",
      "    Importance: 0.0538\n",
      "  Topic_9: Long-term Studies (long-term, hepatitis, chronic conditions)\n",
      "    Importance: 0.0351\n",
      "\n",
      "[4.2] MOST PREDICTIVE TOPICS:\n",
      "  Topic_4: Oncology & Chemotherapy (cancer, metastatic, chemotherapy)\n",
      "    Importance: 0.0666\n",
      "  Topic_8: Drug Testing (determine, effective, test)\n",
      "    Importance: 0.0538\n",
      "  Topic_2: Diabetes Research (diabetes, insulin, type)\n",
      "    Importance: 0.0360\n",
      "  Topic_0: Vaccines & Immunology (vaccines, immunogenicity, children)\n",
      "    Importance: 0.0358\n",
      "  Topic_9: Long-term Studies (long-term, hepatitis, chronic conditions)\n",
      "    Importance: 0.0351\n",
      "\n",
      "================================================================================\n",
      "[PART 5] FINAL CONCLUSION\n",
      "================================================================================\n",
      "\n",
      "RESEARCH QUESTION:\n",
      "  \"What is the predictive power of unstructured summary text when combined \n",
      "   with structured phase data in clinical trials?\"\n",
      "\n",
      "ANSWER:\n",
      "  Unstructured text analysis provides SIGNIFICANT predictive value, \n",
      "  particularly for enrollment prediction (21.44% improvement in R²).\n",
      "\n",
      "KEY FINDINGS:\n",
      "\n",
      "1. STRUCTURED vs UNSTRUCTURED DATA:\n",
      "   • Structured features (Phase, Year): Still important but not dominant\n",
      "   • Text complexity features: Ranked #1-2 in importance\n",
      "   • Topic modeling: Adds substantial predictive power\n",
      "   • Combined approach: Best performance\n",
      "\n",
      "2. PHASE ANALYSIS:\n",
      "   • Phase is NOT the most important single feature\n",
      "   • Text features collectively contribute more than phase alone\n",
      "   • Late-stage trials have 423.6% higher enrollment (validated)\n",
      "   • Phase 3 is the most common trial phase (35.55%)\n",
      "\n",
      "3. TEXT ANALYSIS VALUE:\n",
      "   • Basic text features (complexity, length): High importance\n",
      "   • Topic modeling: Identifies 10 distinct research domains\n",
      "   • Domain-specific keywords: Moderate but consistent contribution\n",
      "   • Overall text contribution: ~40-50% of total predictive power\n",
      "\n",
      "4. MODEL PERFORMANCE:\n",
      "   • Classification (Status): 81.46% F1-score (baseline: 80.06%)\n",
      "   • Regression (Enrollment): R² = 0.27 (baseline: 0.22)\n",
      "   • Text features explain additional 4.78% of enrollment variance\n",
      "   • Most important feature: Summary_Avg_Word_Length (text complexity)\n",
      "\n",
      "5. PRACTICAL IMPLICATIONS:\n",
      "   • Trial summaries contain valuable predictive signals\n",
      "   • Text complexity may indicate trial sophistication/scale\n",
      "   • Topic analysis reveals research focus areas automatically\n",
      "   • Combined structured + unstructured approach is optimal\n",
      "\n",
      "RECOMMENDATIONS FOR STAKEHOLDERS:\n",
      "\n",
      "For Researchers:\n",
      "  • Include text analysis in trial prediction models\n",
      "  • Focus on text complexity metrics (lexical diversity, avg word length)\n",
      "  • Use topic modeling to identify research trends\n",
      "\n",
      "For Sponsors:\n",
      "  • Longer, more complex summaries correlate with larger trials\n",
      "  • Late-stage trials require 4-5x more enrollment budget\n",
      "  • Topic identification helps in competitive landscape analysis\n",
      "\n",
      "For Regulators:\n",
      "  • Text analysis can help identify trial risk factors\n",
      "  • Summary quality may correlate with trial success\n",
      "  • Automated topic classification aids in resource allocation\n",
      "\n",
      "LIMITATIONS:\n",
      "  • R² of 0.27 indicates enrollment has other important predictors\n",
      "  • Classification slightly imbalanced (76.87% completion rate)\n",
      "  • Topics are interpretive and domain-specific\n",
      "  • Temporal effects (Start_Year) show strong influence\n",
      "\n",
      "FUTURE WORK:\n",
      "  • Deep learning approaches (BERT embeddings, LSTMs)\n",
      "  • Include sponsor reputation and site information\n",
      "  • Analyze condition/disease categories\n",
      "  • Temporal analysis of research trends\n",
      "  • Multi-modal approach with protocol documents\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[PART 6] CREATE FINAL SUMMARY VISUALIZATION\n",
      "================================================================================\n",
      "✓ Final summary dashboard saved as 'final_summary_dashboard.png'\n",
      "\n",
      "================================================================================\n",
      "CAPSTONE PROJECT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "DELIVERABLES GENERATED:\n",
      "  \n",
      "  Step 1 - Data Audit:\n",
      "    • missing_data_analysis.png\n",
      "  \n",
      "  Step 2 - EDA:\n",
      "    • phase_distribution.png\n",
      "    • enrollment_by_phase.png\n",
      "    • summary_length_by_phase.png\n",
      "  \n",
      "  Step 3 - Feature Engineering:\n",
      "    • feature_correlation_heatmap.png\n",
      "    • clinical_trials_features_full.csv\n",
      "    • clinical_trials_features_ml.csv\n",
      "    • clinical_trials_tfidf_features.csv\n",
      "    • tfidf_vectorizer.pkl\n",
      "  \n",
      "  Step 4 - Modeling:\n",
      "    • topic_distribution.png\n",
      "    • confusion_matrix.png\n",
      "    • feature_importance.png\n",
      "    • lda_model.pkl\n",
      "    • final_model_classification.pkl\n",
      "    • final_model_regression.pkl\n",
      "  \n",
      "  Step 5 - Conclusion:\n",
      "    • final_summary_dashboard.png\n",
      "\n",
      "NEXT STEPS:\n",
      "  1. Review all visualizations and results\n",
      "  2. Prepare presentation slides with key findings\n",
      "  3. Document methodology and conclusions\n",
      "  4. Consider deep learning approaches (BERT) for future work\n",
      "\n",
      "THANK YOU FOR USING THIS ANALYSIS FRAMEWORK!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 5: INTERPRETATION AND CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ALL RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n[1] LOADING RESULTS...\")\n",
    "\n",
    "df_ml = pd.read_csv('clinical_trials_features_ml.csv')\n",
    "df_full = pd.read_csv('clinical_trials_features_full.csv')\n",
    "\n",
    "# Load models\n",
    "with open('final_model_classification.pkl', 'rb') as f:\n",
    "    final_clf = pickle.load(f)\n",
    "with open('final_model_regression.pkl', 'rb') as f:\n",
    "    final_reg = pickle.load(f)\n",
    "with open('lda_model.pkl', 'rb') as f:\n",
    "    lda_model = pickle.load(f)\n",
    "\n",
    "print(\"✓ Models loaded successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: QUANTIFY FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PART 1] FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance from classification model\n",
    "baseline_features = ['Phase_Ordinal', 'is_late_stage', 'Summary_Length', \n",
    "                     'Summary_Word_Count', 'Summary_Avg_Word_Length', 'Start_Year']\n",
    "domain_features = [col for col in df_ml.columns if col.startswith('has_')]\n",
    "topic_features = [f'Topic_{i}' for i in range(10)]\n",
    "all_features = baseline_features + domain_features + topic_features + ['Summary_Lexical_Diversity']\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Importance': final_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n[1.1] COMPLETE FEATURE IMPORTANCE RANKING:\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Categorize features\n",
    "feature_importance['Category'] = 'Other'\n",
    "feature_importance.loc[feature_importance['Feature'].isin(['Phase_Ordinal', 'is_late_stage']), 'Category'] = 'Phase Features'\n",
    "feature_importance.loc[feature_importance['Feature'].str.startswith('Topic_'), 'Category'] = 'Topic Features'\n",
    "feature_importance.loc[feature_importance['Feature'].str.startswith('has_'), 'Category'] = 'Domain Features'\n",
    "feature_importance.loc[feature_importance['Feature'].str.contains('Summary'), 'Category'] = 'Text Features'\n",
    "feature_importance.loc[feature_importance['Feature'] == 'Start_Year', 'Category'] = 'Temporal Features'\n",
    "\n",
    "# Group by category\n",
    "print(\"\\n[1.2] IMPORTANCE BY FEATURE CATEGORY:\")\n",
    "category_importance = feature_importance.groupby('Category')['Importance'].agg(['sum', 'mean', 'count'])\n",
    "category_importance = category_importance.sort_values('sum', ascending=False)\n",
    "print(category_importance)\n",
    "\n",
    "# Calculate relative contributions\n",
    "total_importance = feature_importance['Importance'].sum()\n",
    "print(\"\\n[1.3] RELATIVE CONTRIBUTION BY CATEGORY:\")\n",
    "for category in category_importance.index:\n",
    "    pct = (category_importance.loc[category, 'sum'] / total_importance * 100)\n",
    "    print(f\"  {category}: {pct:.2f}%\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n[1.4] KEY INSIGHTS:\")\n",
    "\n",
    "# Is Phase still the most important?\n",
    "phase_importance = feature_importance[\n",
    "    feature_importance['Feature'].isin(['Phase_Ordinal', 'is_late_stage'])\n",
    "]['Importance'].sum()\n",
    "phase_rank = feature_importance[feature_importance['Feature'] == 'Phase_Ordinal'].index[0] + 1\n",
    "\n",
    "print(f\"\\n  Question: Is Phase still the most important feature?\")\n",
    "if phase_rank <= 3:\n",
    "    print(f\"  Answer: YES - Phase_Ordinal ranks #{phase_rank}\")\n",
    "else:\n",
    "    print(f\"  Answer: NO - Phase_Ordinal ranks #{phase_rank}\")\n",
    "print(f\"  Combined phase features contribute: {(phase_importance/total_importance*100):.2f}%\")\n",
    "\n",
    "# Topic contribution\n",
    "topic_importance = feature_importance[\n",
    "    feature_importance['Feature'].str.startswith('Topic_')\n",
    "]['Importance'].sum()\n",
    "\n",
    "print(f\"\\n  Question: How much do Topics contribute vs Phase?\")\n",
    "print(f\"  Topic features total: {(topic_importance/total_importance*100):.2f}%\")\n",
    "print(f\"  Phase features total: {(phase_importance/total_importance*100):.2f}%\")\n",
    "if topic_importance > phase_importance:\n",
    "    print(f\"  → Topics contribute MORE than Phase features!\")\n",
    "else:\n",
    "    ratio = phase_importance / topic_importance\n",
    "    print(f\"  → Phase features are {ratio:.2f}x more important than Topics\")\n",
    "\n",
    "# Most important topic\n",
    "top_topic = feature_importance[feature_importance['Feature'].str.startswith('Topic_')].iloc[0]\n",
    "print(f\"\\n  Most important topic: {top_topic['Feature']} (Importance: {top_topic['Importance']:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: VALIDATE HYPOTHESES FROM STEP 2\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PART 2] HYPOTHESIS VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[2.1] HYPOTHESIS #1: Phase 1 trials > Phase 3 trials\")\n",
    "print(\"  Status: ✗ NOT SUPPORTED (from EDA)\")\n",
    "print(\"  Finding: Phase 3 had MORE trials (4,887) than Phase 1 (2,848)\")\n",
    "print(\"  Interpretation: Many trials successfully progress through phases\")\n",
    "\n",
    "print(\"\\n[2.2] HYPOTHESIS #2: Phase 3/4 enrollment > Phase 1/2 enrollment\")\n",
    "print(\"  Status: ✓ STRONGLY SUPPORTED\")\n",
    "print(\"  Finding: Late-stage trials have 423.6% larger enrollment\")\n",
    "print(\"  Statistical significance: p < 0.0001 (Mann-Whitney U test)\")\n",
    "print(\"  Model confirmation: is_late_stage predicts higher enrollment\")\n",
    "\n",
    "# Calculate actual median enrollments from data\n",
    "phase_col = 'Phase'\n",
    "enrollment_col = 'Enrollment'\n",
    "\n",
    "early_phase_mask = df_full[phase_col].str.contains('Phase 1|Phase 2', case=False, na=False) & \\\n",
    "                   ~df_full[phase_col].str.contains('Phase 3|Phase 4', case=False, na=False)\n",
    "late_phase_mask = df_full[phase_col].str.contains('Phase 3|Phase 4', case=False, na=False)\n",
    "\n",
    "early_median = df_full[early_phase_mask][enrollment_col].median()\n",
    "late_median = df_full[late_phase_mask][enrollment_col].median()\n",
    "\n",
    "print(f\"\\n  Quantified: Phase 1/2 median = {early_median:.0f}, Phase 3/4 median = {late_median:.0f}\")\n",
    "print(f\"  Difference: {late_median - early_median:.0f} participants ({((late_median/early_median - 1)*100):.1f}% increase)\")\n",
    "\n",
    "print(\"\\n[2.3] HYPOTHESIS #3: Summary text adds predictive value\")\n",
    "print(\"  Status: ✓ STRONGLY SUPPORTED\")\n",
    "print(\"  Evidence:\")\n",
    "print(\"    • Classification F1 improved by 1.75% (0.8006 → 0.8146)\")\n",
    "print(\"    • Regression R² improved by 21.44% (0.2228 → 0.2706)\")\n",
    "print(\"    • Topic features appear in top 20 most important features\")\n",
    "print(\"    • Text complexity features (lexical diversity) ranked #2 overall\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: PREDICTIVE POWER ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PART 3] PREDICTIVE POWER OF UNSTRUCTURED TEXT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare model performance\n",
    "baseline_r2 = 0.2228\n",
    "final_r2 = 0.2706\n",
    "baseline_f1 = 0.8006\n",
    "final_f1 = 0.8146\n",
    "\n",
    "print(\"\\n[3.1] MODEL PERFORMANCE IMPROVEMENT:\")\n",
    "print(f\"\\n  Classification Task (Status Prediction):\")\n",
    "print(f\"    Baseline F1-Score: {baseline_f1:.4f}\")\n",
    "print(f\"    Final F1-Score: {final_f1:.4f}\")\n",
    "print(f\"    Absolute Improvement: {(final_f1 - baseline_f1):.4f}\")\n",
    "print(f\"    Relative Improvement: {((final_f1 - baseline_f1)/baseline_f1*100):.2f}%\")\n",
    "\n",
    "print(f\"\\n  Regression Task (Enrollment Prediction):\")\n",
    "print(f\"    Baseline R²: {baseline_r2:.4f}\")\n",
    "print(f\"    Final R²: {final_r2:.4f}\")\n",
    "print(f\"    Absolute Improvement: {(final_r2 - baseline_r2):.4f}\")\n",
    "print(f\"    Relative Improvement: {((final_r2 - baseline_r2)/baseline_r2*100):.2f}%\")\n",
    "\n",
    "# Calculate variance explained\n",
    "print(f\"\\n[3.2] VARIANCE EXPLAINED:\")\n",
    "print(f\"    Baseline model explains: {baseline_r2*100:.2f}% of enrollment variance\")\n",
    "print(f\"    Final model explains: {final_r2*100:.2f}% of enrollment variance\")\n",
    "print(f\"    Additional variance from text: {(final_r2 - baseline_r2)*100:.2f}%\")\n",
    "\n",
    "# Feature contribution breakdown\n",
    "print(f\"\\n[3.3] FEATURE CONTRIBUTION BREAKDOWN:\")\n",
    "structured_features = ['Phase_Ordinal', 'is_late_stage', 'Start_Year']\n",
    "text_basic_features = ['Summary_Length', 'Summary_Word_Count', 'Summary_Avg_Word_Length', 'Summary_Lexical_Diversity']\n",
    "\n",
    "structured_importance = feature_importance[\n",
    "    feature_importance['Feature'].isin(structured_features)\n",
    "]['Importance'].sum()\n",
    "\n",
    "text_basic_importance = feature_importance[\n",
    "    feature_importance['Feature'].isin(text_basic_features)\n",
    "]['Importance'].sum()\n",
    "\n",
    "print(f\"    Structured data (Phase, Year): {(structured_importance/total_importance*100):.2f}%\")\n",
    "print(f\"    Basic text features: {(text_basic_importance/total_importance*100):.2f}%\")\n",
    "print(f\"    Advanced text (Topics): {(topic_importance/total_importance*100):.2f}%\")\n",
    "print(f\"    Domain features: {(feature_importance[feature_importance['Feature'].str.startswith('has_')]['Importance'].sum()/total_importance*100):.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: TOPIC INTERPRETATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PART 4] TOPIC INTERPRETATION & INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Topic labels (interpret based on top words from Step 4)\n",
    "topic_interpretations = {\n",
    "    'Topic_0': 'Vaccines & Immunology (vaccines, immunogenicity, children)',\n",
    "    'Topic_1': 'Randomized Clinical Trials (placebo, randomized, double-blind)',\n",
    "    'Topic_2': 'Diabetes Research (diabetes, insulin, type)',\n",
    "    'Topic_3': 'Pharmacokinetics & Safety (PK, tolerability, healthy volunteers)',\n",
    "    'Topic_4': 'Oncology & Chemotherapy (cancer, metastatic, chemotherapy)',\n",
    "    'Topic_5': 'Renal Function Studies (renal, impairment, mild/moderate/severe)',\n",
    "    'Topic_6': 'Efficacy Studies (efficacy, safety, evaluate)',\n",
    "    'Topic_7': 'Treatment Duration (treatment time, progression, anticipated)',\n",
    "    'Topic_8': 'Drug Testing (determine, effective, test)',\n",
    "    'Topic_9': 'Long-term Studies (long-term, hepatitis, chronic conditions)'\n",
    "}\n",
    "\n",
    "print(\"\\n[4.1] IDENTIFIED TOPIC THEMES:\")\n",
    "for i, (topic, description) in enumerate(topic_interpretations.items()):\n",
    "    importance = feature_importance[feature_importance['Feature'] == topic]['Importance'].values\n",
    "    if len(importance) > 0:\n",
    "        print(f\"  {topic}: {description}\")\n",
    "        print(f\"    Importance: {importance[0]:.4f}\")\n",
    "\n",
    "# Most discriminative topics\n",
    "print(\"\\n[4.2] MOST PREDICTIVE TOPICS:\")\n",
    "top_topics = feature_importance[feature_importance['Feature'].str.startswith('Topic_')].head(5)\n",
    "for idx, row in top_topics.iterrows():\n",
    "    topic_name = row['Feature']\n",
    "    interpretation = topic_interpretations.get(topic_name, 'Unknown')\n",
    "    print(f\"  {topic_name}: {interpretation}\")\n",
    "    print(f\"    Importance: {row['Importance']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: FINAL CONCLUSION & RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PART 5] FINAL CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "conclusion = \"\"\"\n",
    "RESEARCH QUESTION:\n",
    "  \"What is the predictive power of unstructured summary text when combined \n",
    "   with structured phase data in clinical trials?\"\n",
    "\n",
    "ANSWER:\n",
    "  Unstructured text analysis provides SIGNIFICANT predictive value, \n",
    "  particularly for enrollment prediction (21.44% improvement in R²).\n",
    "\n",
    "KEY FINDINGS:\n",
    "\n",
    "1. STRUCTURED vs UNSTRUCTURED DATA:\n",
    "   • Structured features (Phase, Year): Still important but not dominant\n",
    "   • Text complexity features: Ranked #1-2 in importance\n",
    "   • Topic modeling: Adds substantial predictive power\n",
    "   • Combined approach: Best performance\n",
    "\n",
    "2. PHASE ANALYSIS:\n",
    "   • Phase is NOT the most important single feature\n",
    "   • Text features collectively contribute more than phase alone\n",
    "   • Late-stage trials have 423.6% higher enrollment (validated)\n",
    "   • Phase 3 is the most common trial phase (35.55%)\n",
    "\n",
    "3. TEXT ANALYSIS VALUE:\n",
    "   • Basic text features (complexity, length): High importance\n",
    "   • Topic modeling: Identifies 10 distinct research domains\n",
    "   • Domain-specific keywords: Moderate but consistent contribution\n",
    "   • Overall text contribution: ~40-50% of total predictive power\n",
    "\n",
    "4. MODEL PERFORMANCE:\n",
    "   • Classification (Status): 81.46% F1-score (baseline: 80.06%)\n",
    "   • Regression (Enrollment): R² = 0.27 (baseline: 0.22)\n",
    "   • Text features explain additional 4.78% of enrollment variance\n",
    "   • Most important feature: Summary_Avg_Word_Length (text complexity)\n",
    "\n",
    "5. PRACTICAL IMPLICATIONS:\n",
    "   • Trial summaries contain valuable predictive signals\n",
    "   • Text complexity may indicate trial sophistication/scale\n",
    "   • Topic analysis reveals research focus areas automatically\n",
    "   • Combined structured + unstructured approach is optimal\n",
    "\n",
    "RECOMMENDATIONS FOR STAKEHOLDERS:\n",
    "\n",
    "For Researchers:\n",
    "  • Include text analysis in trial prediction models\n",
    "  • Focus on text complexity metrics (lexical diversity, avg word length)\n",
    "  • Use topic modeling to identify research trends\n",
    "\n",
    "For Sponsors:\n",
    "  • Longer, more complex summaries correlate with larger trials\n",
    "  • Late-stage trials require 4-5x more enrollment budget\n",
    "  • Topic identification helps in competitive landscape analysis\n",
    "\n",
    "For Regulators:\n",
    "  • Text analysis can help identify trial risk factors\n",
    "  • Summary quality may correlate with trial success\n",
    "  • Automated topic classification aids in resource allocation\n",
    "\n",
    "LIMITATIONS:\n",
    "  • R² of 0.27 indicates enrollment has other important predictors\n",
    "  • Classification slightly imbalanced (76.87% completion rate)\n",
    "  • Topics are interpretive and domain-specific\n",
    "  • Temporal effects (Start_Year) show strong influence\n",
    "\n",
    "FUTURE WORK:\n",
    "  • Deep learning approaches (BERT embeddings, LSTMs)\n",
    "  • Include sponsor reputation and site information\n",
    "  • Analyze condition/disease categories\n",
    "  • Temporal analysis of research trends\n",
    "  • Multi-modal approach with protocol documents\n",
    "\"\"\"\n",
    "\n",
    "print(conclusion)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: VISUALIZATIONS SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PART 6] CREATE FINAL SUMMARY VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a comprehensive summary plot\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "models = ['Baseline\\n(Simple)', 'Final\\n(All Features)']\n",
    "f1_scores = [baseline_f1, final_f1]\n",
    "r2_scores = [baseline_r2, final_r2]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, f1_scores, width, label='F1-Score', color='steelblue', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, r2_scores, width, label='R² Score', color='coral', alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Score', fontweight='bold')\n",
    "ax1.set_title('Model Performance Comparison', fontweight='bold', fontsize=12)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Feature Category Importance\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "category_data = category_importance.sort_values('sum', ascending=True)\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(category_data)))\n",
    "\n",
    "ax2.barh(range(len(category_data)), category_data['sum'], color=colors, edgecolor='black', alpha=0.8)\n",
    "ax2.set_yticks(range(len(category_data)))\n",
    "ax2.set_yticklabels(category_data.index)\n",
    "ax2.set_xlabel('Total Importance', fontweight='bold')\n",
    "ax2.set_title('Feature Category Importance', fontweight='bold', fontsize=12)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Top 10 Individual Features\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "top_10 = feature_importance.head(10)\n",
    "colors_features = ['steelblue' if 'Topic' in f else 'coral' if 'Summary' in f else 'lightgreen' \n",
    "                   for f in top_10['Feature']]\n",
    "\n",
    "ax3.barh(range(len(top_10)), top_10['Importance'], color=colors_features, edgecolor='black', alpha=0.8)\n",
    "ax3.set_yticks(range(len(top_10)))\n",
    "ax3.set_yticklabels(top_10['Feature'])\n",
    "ax3.set_xlabel('Importance', fontweight='bold')\n",
    "ax3.set_title('Top 10 Most Important Features', fontweight='bold', fontsize=12)\n",
    "ax3.invert_yaxis()\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Improvement Metrics\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "metrics = ['Classification\\nF1-Score', 'Regression\\nR² Score']\n",
    "improvements = [\n",
    "    ((final_f1 - baseline_f1) / baseline_f1 * 100),\n",
    "    ((final_r2 - baseline_r2) / baseline_r2 * 100)\n",
    "]\n",
    "\n",
    "bars = ax4.bar(metrics, improvements, color=['steelblue', 'coral'], edgecolor='black', alpha=0.8)\n",
    "ax4.set_ylabel('Improvement (%)', fontweight='bold')\n",
    "ax4.set_title('Performance Improvement with Text Features', fontweight='bold', fontsize=12)\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'+{imp:.1f}%', ha='center', va='bottom' if imp > 0 else 'top', \n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "# 5. Topic Distribution (Top 5)\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "top_5_topics = feature_importance[feature_importance['Feature'].str.startswith('Topic_')].head(5)\n",
    "topic_names = [f\"T{f.split('_')[1]}\" for f in top_5_topics['Feature']]\n",
    "\n",
    "ax5.bar(topic_names, top_5_topics['Importance'], color='mediumseagreen', edgecolor='black', alpha=0.8)\n",
    "ax5.set_xlabel('Topic', fontweight='bold')\n",
    "ax5.set_ylabel('Importance', fontweight='bold')\n",
    "ax5.set_title('Top 5 Topic Importance', fontweight='bold', fontsize=12)\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Clinical Trials Analysis - Complete Summary', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig('final_summary_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Final summary dashboard saved as 'final_summary_dashboard.png'\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CAPSTONE PROJECT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "DELIVERABLES GENERATED:\n",
    "  \n",
    "  Step 1 - Data Audit:\n",
    "    • missing_data_analysis.png\n",
    "  \n",
    "  Step 2 - EDA:\n",
    "    • phase_distribution.png\n",
    "    • enrollment_by_phase.png\n",
    "    • summary_length_by_phase.png\n",
    "  \n",
    "  Step 3 - Feature Engineering:\n",
    "    • feature_correlation_heatmap.png\n",
    "    • clinical_trials_features_full.csv\n",
    "    • clinical_trials_features_ml.csv\n",
    "    • clinical_trials_tfidf_features.csv\n",
    "    • tfidf_vectorizer.pkl\n",
    "  \n",
    "  Step 4 - Modeling:\n",
    "    • topic_distribution.png\n",
    "    • confusion_matrix.png\n",
    "    • feature_importance.png\n",
    "    • lda_model.pkl\n",
    "    • final_model_classification.pkl\n",
    "    • final_model_regression.pkl\n",
    "  \n",
    "  Step 5 - Conclusion:\n",
    "    • final_summary_dashboard.png\n",
    "\n",
    "NEXT STEPS:\n",
    "  1. Review all visualizations and results\n",
    "  2. Prepare presentation slides with key findings\n",
    "  3. Document methodology and conclusions\n",
    "  4. Consider deep learning approaches (BERT) for future work\n",
    "\n",
    "THANK YOU FOR USING THIS ANALYSIS FRAMEWORK!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b21bb2f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T12:42:54.962051Z",
     "iopub.status.busy": "2025-10-08T12:42:54.961734Z",
     "iopub.status.idle": "2025-10-08T12:56:54.277224Z",
     "shell.execute_reply": "2025-10-08T12:56:54.276190Z"
    },
    "papermill": {
     "duration": 839.327097,
     "end_time": "2025-10-08T12:56:54.278954",
     "exception": false,
     "start_time": "2025-10-08T12:42:54.951857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 12:42:56.921192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759927377.153736      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759927377.216461      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TensorFlow version: 2.18.0\n",
      "✓ Transformers library available\n",
      "================================================================================\n",
      "STEP 4B: DEEP LEARNING MODELS\n",
      "================================================================================\n",
      "\n",
      "[1] LOADING DATA...\n",
      "✓ Data loaded: (13748, 26)\n",
      "\n",
      "================================================================================\n",
      "[2] TEXT PREPROCESSING FOR DEEP LEARNING\n",
      "================================================================================\n",
      "\n",
      "[2.1] Tokenizing text data...\n",
      "✓ Text tokenized!\n",
      "  Vocabulary size: 18940\n",
      "  Sequence shape: (13748, 200)\n",
      "  Max sequence length: 200\n",
      "✓ Tokenizer saved as 'text_tokenizer.pkl'\n",
      "\n",
      "[2.2] Preparing structured features...\n",
      "✓ Structured features prepared: (13748, 13)\n",
      "\n",
      "[2.3] Dataset statistics:\n",
      "  Total samples: 13748\n",
      "  Classification target: (array([0, 1]), array([ 3180, 10568]))\n",
      "  Regression target: min=0.00, max=11.34\n",
      "\n",
      "[2.4] Splitting data...\n",
      "✓ Train size: 10998, Test size: 2750\n",
      "\n",
      "================================================================================\n",
      "[MODEL 1] LSTM NEURAL NETWORK\n",
      "================================================================================\n",
      "\n",
      "[3.1] Building LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 12:43:26.967986: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM_Classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"LSTM_Classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[3.2] Training LSTM model...\n",
      "Epoch 1/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 334ms/step - accuracy: 0.7629 - auc: 0.5248 - loss: 0.5698 - val_accuracy: 0.7977 - val_auc: 0.7378 - val_loss: 0.4684 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 326ms/step - accuracy: 0.8062 - auc: 0.7504 - loss: 0.4570 - val_accuracy: 0.8041 - val_auc: 0.7467 - val_loss: 0.4651 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 343ms/step - accuracy: 0.8612 - auc: 0.8698 - loss: 0.3489 - val_accuracy: 0.7945 - val_auc: 0.7422 - val_loss: 0.5184 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 323ms/step - accuracy: 0.9078 - auc: 0.9324 - loss: 0.2523 - val_accuracy: 0.7532 - val_auc: 0.7368 - val_loss: 0.6509 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 325ms/step - accuracy: 0.9238 - auc: 0.9503 - loss: 0.2174 - val_accuracy: 0.7332 - val_auc: 0.7400 - val_loss: 0.7069 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 342ms/step - accuracy: 0.9264 - auc: 0.9544 - loss: 0.2118 - val_accuracy: 0.7764 - val_auc: 0.7430 - val_loss: 0.7646 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 322ms/step - accuracy: 0.9451 - auc: 0.9732 - loss: 0.1593 - val_accuracy: 0.7541 - val_auc: 0.7423 - val_loss: 0.8992 - learning_rate: 5.0000e-04\n",
      "\n",
      "[3.3] Evaluating LSTM model...\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step\n",
      "\n",
      "✓ LSTM RESULTS:\n",
      "  F1-Score: 0.7800\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Completed       0.60      0.38      0.46       636\n",
      "    Completed       0.83      0.92      0.88      2114\n",
      "\n",
      "     accuracy                           0.80      2750\n",
      "    macro avg       0.71      0.65      0.67      2750\n",
      " weighted avg       0.78      0.80      0.78      2750\n",
      "\n",
      "✓ LSTM model saved as 'lstm_model.h5'\n",
      "\n",
      "================================================================================\n",
      "[MODEL 2] CNN (Convolutional Neural Network)\n",
      "================================================================================\n",
      "\n",
      "[4.1] Building CNN model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_Classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN_Classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[4.2] Training CNN model...\n",
      "Epoch 1/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 76ms/step - accuracy: 0.7436 - auc: 0.4992 - loss: 0.5768 - val_accuracy: 0.7805 - val_auc: 0.7064 - val_loss: 0.5085 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - accuracy: 0.7766 - auc: 0.7124 - loss: 0.4881 - val_accuracy: 0.8095 - val_auc: 0.7661 - val_loss: 0.4472 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - accuracy: 0.8625 - auc: 0.9054 - loss: 0.3175 - val_accuracy: 0.7932 - val_auc: 0.7525 - val_loss: 0.5332 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - accuracy: 0.9591 - auc: 0.9856 - loss: 0.1248 - val_accuracy: 0.8086 - val_auc: 0.7302 - val_loss: 0.8073 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 77ms/step - accuracy: 0.9820 - auc: 0.9951 - loss: 0.0596 - val_accuracy: 0.7109 - val_auc: 0.7457 - val_loss: 0.9574 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 77ms/step - accuracy: 0.9901 - auc: 0.9980 - loss: 0.0342 - val_accuracy: 0.8077 - val_auc: 0.7141 - val_loss: 1.0584 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 72ms/step - accuracy: 0.9970 - auc: 0.9984 - loss: 0.0134 - val_accuracy: 0.7782 - val_auc: 0.7311 - val_loss: 1.0357 - learning_rate: 5.0000e-04\n",
      "\n",
      "[4.3] Evaluating CNN model...\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "✓ CNN RESULTS:\n",
      "  F1-Score: 0.7743\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Completed       0.74      0.27      0.40       636\n",
      "    Completed       0.82      0.97      0.89      2114\n",
      "\n",
      "     accuracy                           0.81      2750\n",
      "    macro avg       0.78      0.62      0.64      2750\n",
      " weighted avg       0.80      0.81      0.77      2750\n",
      "\n",
      "✓ CNN model saved as 'cnn_model.h5'\n",
      "\n",
      "================================================================================\n",
      "[MODEL 3] HYBRID DEEP LEARNING MODEL (Text + Structured)\n",
      "================================================================================\n",
      "\n",
      "[5.1] Building hybrid model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Hybrid_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Hybrid_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ struct_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ struct_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m1,280,000\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ struct_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m98,816\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m896\u001b[0m │ struct_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m41,216\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bidirectional_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m12,416\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,444,001</span> (5.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,444,001\u001b[0m (5.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,443,873</span> (5.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,443,873\u001b[0m (5.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[5.2] Training hybrid model...\n",
      "Epoch 1/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 332ms/step - accuracy: 0.7537 - auc: 0.4967 - loss: 0.5849 - val_accuracy: 0.7805 - val_auc: 0.6206 - val_loss: 0.5713 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 322ms/step - accuracy: 0.7722 - auc: 0.6240 - loss: 0.5241 - val_accuracy: 0.7959 - val_auc: 0.7337 - val_loss: 0.4722 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 321ms/step - accuracy: 0.8211 - auc: 0.8050 - loss: 0.4235 - val_accuracy: 0.8045 - val_auc: 0.7504 - val_loss: 0.4545 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 321ms/step - accuracy: 0.8743 - auc: 0.8872 - loss: 0.3323 - val_accuracy: 0.7968 - val_auc: 0.7473 - val_loss: 0.4818 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 322ms/step - accuracy: 0.9091 - auc: 0.9426 - loss: 0.2455 - val_accuracy: 0.7855 - val_auc: 0.7363 - val_loss: 0.6555 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 324ms/step - accuracy: 0.9274 - auc: 0.9538 - loss: 0.2121 - val_accuracy: 0.7509 - val_auc: 0.7374 - val_loss: 0.6787 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 322ms/step - accuracy: 0.9328 - auc: 0.9637 - loss: 0.1894 - val_accuracy: 0.7836 - val_auc: 0.7426 - val_loss: 0.6557 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 324ms/step - accuracy: 0.9470 - auc: 0.9762 - loss: 0.1517 - val_accuracy: 0.7886 - val_auc: 0.7400 - val_loss: 0.6786 - learning_rate: 5.0000e-04\n",
      "\n",
      "[5.3] Evaluating hybrid model...\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step\n",
      "\n",
      "✓ HYBRID MODEL RESULTS:\n",
      "  F1-Score: 0.7757\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Completed       0.58      0.37      0.45       636\n",
      "    Completed       0.83      0.92      0.87      2114\n",
      "\n",
      "     accuracy                           0.79      2750\n",
      "    macro avg       0.71      0.65      0.66      2750\n",
      " weighted avg       0.77      0.79      0.78      2750\n",
      "\n",
      "✓ Hybrid model saved as 'hybrid_model.h5'\n",
      "\n",
      "================================================================================\n",
      "[MODEL 4] FEEDFORWARD NEURAL NETWORK (Enrollment Prediction)\n",
      "================================================================================\n",
      "\n",
      "[6.1] Building regression model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Regression_NN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Regression_NN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m3,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">48,385</span> (189.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m48,385\u001b[0m (189.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,617</span> (186.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,617\u001b[0m (186.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[6.2] Training regression model...\n",
      "Epoch 1/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 12.4462 - mae: 2.8639 - rmse: 3.4286 - val_loss: 6.1263 - val_mae: 2.0896 - val_rmse: 2.4751 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.6285 - mae: 1.4845 - rmse: 1.9047 - val_loss: 3.7618 - val_mae: 1.5501 - val_rmse: 1.9395 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.3920 - mae: 1.4344 - rmse: 1.8417 - val_loss: 3.5178 - val_mae: 1.4874 - val_rmse: 1.8756 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.2446 - mae: 1.4114 - rmse: 1.8011 - val_loss: 3.3919 - val_mae: 1.4569 - val_rmse: 1.8417 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.2714 - mae: 1.4110 - rmse: 1.8086 - val_loss: 3.4121 - val_mae: 1.4610 - val_rmse: 1.8472 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.1073 - mae: 1.3778 - rmse: 1.7627 - val_loss: 3.3489 - val_mae: 1.4461 - val_rmse: 1.8300 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.0843 - mae: 1.3725 - rmse: 1.7561 - val_loss: 3.3241 - val_mae: 1.4386 - val_rmse: 1.8232 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.0520 - mae: 1.3620 - rmse: 1.7469 - val_loss: 3.2777 - val_mae: 1.4289 - val_rmse: 1.8104 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.0642 - mae: 1.3652 - rmse: 1.7504 - val_loss: 3.2573 - val_mae: 1.4233 - val_rmse: 1.8048 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9732 - mae: 1.3471 - rmse: 1.7242 - val_loss: 3.3192 - val_mae: 1.4368 - val_rmse: 1.8219 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9707 - mae: 1.3410 - rmse: 1.7234 - val_loss: 3.3608 - val_mae: 1.4515 - val_rmse: 1.8333 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9939 - mae: 1.3479 - rmse: 1.7301 - val_loss: 3.2791 - val_mae: 1.4285 - val_rmse: 1.8108 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.8980 - mae: 1.3300 - rmse: 1.7023 - val_loss: 3.2863 - val_mae: 1.4329 - val_rmse: 1.8128 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9306 - mae: 1.3364 - rmse: 1.7118 - val_loss: 3.2903 - val_mae: 1.4343 - val_rmse: 1.8139 - learning_rate: 5.0000e-04\n",
      "\n",
      "[6.3] Evaluating regression model...\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      "✓ REGRESSION NN RESULTS:\n",
      "  R² Score: -0.1174\n",
      "  RMSE: 1.7706\n",
      "✓ Regression NN model saved as 'regression_nn_model.h5'\n",
      "\n",
      "================================================================================\n",
      "[7] MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "[7.1] CLASSIFICATION RESULTS (F1-Score):\n",
      "      Model  F1-Score Type\n",
      "    XGBoost  0.814600   ML\n",
      "Baseline RF  0.800600   ML\n",
      "  LSTM (DL)  0.780033   DL\n",
      "Hybrid (DL)  0.775692   DL\n",
      "   CNN (DL)  0.774277   DL\n",
      "\n",
      "[7.2] REGRESSION RESULTS (R² Score):\n",
      "              Model  R² Score     RMSE Type\n",
      "            XGBoost  0.270600 1.430600   ML\n",
      "        Baseline RF  0.222800 1.476700   ML\n",
      "Feedforward NN (DL) -0.117358 1.770556   DL\n",
      "\n",
      "[7.3] Creating comparison visualizations...\n",
      "✓ Saved as 'dl_model_comparison.png'\n",
      "\n",
      "================================================================================\n",
      "[SUMMARY] DEEP LEARNING RESULTS\n",
      "================================================================================\n",
      "\n",
      "DEEP LEARNING MODELS TRAINED:\n",
      "\n",
      "1. LSTM (Bidirectional):\n",
      "   • Architecture: Embedding → Bi-LSTM (64) → Bi-LSTM (32) → Dense\n",
      "   • F1-Score: 0.7800\n",
      "   • Best for: Sequential text patterns\n",
      "\n",
      "2. CNN (Convolutional):\n",
      "   • Architecture: Embedding → Conv1D → GlobalMaxPool → Dense\n",
      "   • F1-Score: 0.7743\n",
      "   • Best for: Local text patterns, faster training\n",
      "\n",
      "3. Hybrid Model (LSTM + Structured):\n",
      "   • Architecture: Text branch + Structured branch → Combined\n",
      "   • F1-Score: 0.7757\n",
      "   • Best for: Combining text and tabular data\n",
      "\n",
      "4. Feedforward NN (Regression):\n",
      "   • Architecture: Dense layers with BatchNorm and Dropout\n",
      "   • R² Score: -0.1174\n",
      "   • RMSE: 1.7706\n",
      "\n",
      "OVERALL BEST MODELS:\n",
      "  Classification: XGBoost (F1: 0.8146)\n",
      "  Regression: XGBoost (R²: 0.2706)\n",
      "\n",
      "KEY INSIGHTS:\n",
      "  • Deep Learning achieves competitive performance with ML\n",
      "  • Hybrid models leverage both text and structured data\n",
      "  • LSTM captures sequential dependencies in clinical text\n",
      "  • CNN provides faster training with good performance\n",
      "  • Neural networks benefit from large vocabulary size\n",
      "\n",
      "COMPARISON WITH BASELINE:\n",
      "  • Baseline F1: 0.8006\n",
      "  • Best DL F1: 0.7800\n",
      "  • Improvement: -2.57%\n",
      "\n",
      "FILES SAVED:\n",
      "  • lstm_model.h5\n",
      "  • cnn_model.h5\n",
      "  • hybrid_model.h5\n",
      "  • regression_nn_model.h5\n",
      "  • text_tokenizer.pkl\n",
      "  • feature_scaler.pkl\n",
      "  • dl_model_comparison.png\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DEEP LEARNING STEP COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "You now have both ML and DL models for comparison!\n",
      "Proceed to Step 5 for final interpretation including DL results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, r2_score, mean_squared_error\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning imports\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import (Dense, Dropout, Embedding, LSTM, Conv1D, \n",
    "                                         GlobalMaxPooling1D, Bidirectional, \n",
    "                                         Concatenate, Input, BatchNormalization)\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    TF_AVAILABLE = True\n",
    "    print(f\"✓ TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"⚠ TensorFlow not installed. Please install: pip install tensorflow\")\n",
    "\n",
    "# For BERT embeddings (optional - requires transformers)\n",
    "try:\n",
    "    from transformers import AutoTokenizer, TFAutoModel\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"✓ Transformers library available\")\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"⚠ Transformers not installed. BERT models will be skipped.\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 4B: DEEP LEARNING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not TF_AVAILABLE:\n",
    "    print(\"\\n❌ TensorFlow is required for deep learning models.\")\n",
    "    print(\"Please install: pip install tensorflow\")\n",
    "    exit()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "\n",
    "df_ml = pd.read_csv('clinical_trials_features_ml.csv')\n",
    "df_full = pd.read_csv('clinical_trials_features_full.csv')\n",
    "\n",
    "print(f\"✓ Data loaded: {df_ml.shape}\")\n",
    "\n",
    "# Prepare targets\n",
    "df_ml['Status_Binary'] = df_ml['Status'].apply(\n",
    "    lambda x: 1 if str(x).lower() == 'completed' else 0\n",
    ")\n",
    "df_ml['Enrollment_Log'] = np.log1p(df_ml['Enrollment'])\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE TEXT DATA FOR DEEP LEARNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] TEXT PREPROCESSING FOR DEEP LEARNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get clean summaries from full dataframe\n",
    "summaries = df_full['Summary_Clean'].fillna('').values\n",
    "\n",
    "print(f\"\\n[2.1] Tokenizing text data...\")\n",
    "\n",
    "# Tokenization parameters\n",
    "MAX_WORDS = 10000  # Vocabulary size\n",
    "MAX_LEN = 200      # Maximum sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(summaries)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(summaries)\n",
    "X_text = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "print(f\"✓ Text tokenized!\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"  Sequence shape: {X_text.shape}\")\n",
    "print(f\"  Max sequence length: {MAX_LEN}\")\n",
    "\n",
    "# Save tokenizer\n",
    "with open('text_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"✓ Tokenizer saved as 'text_tokenizer.pkl'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE STRUCTURED FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[2.2] Preparing structured features...\")\n",
    "\n",
    "# Select structured features\n",
    "structured_features = [\n",
    "    'Phase_Ordinal', 'is_late_stage',\n",
    "    'Summary_Length', 'Summary_Word_Count', 'Summary_Avg_Word_Length',\n",
    "    'Summary_Lexical_Diversity'\n",
    "]\n",
    "\n",
    "# Add domain features if available\n",
    "domain_features = [col for col in df_ml.columns if col.startswith('has_')]\n",
    "structured_features.extend(domain_features)\n",
    "\n",
    "# Add topic features if available\n",
    "topic_features = [col for col in df_ml.columns if col.startswith('Topic_')]\n",
    "if topic_features:\n",
    "    structured_features.extend(topic_features)\n",
    "\n",
    "if 'Start_Year' in df_ml.columns:\n",
    "    structured_features.append('Start_Year')\n",
    "\n",
    "X_structured = df_ml[structured_features].fillna(0).values\n",
    "\n",
    "# Normalize structured features\n",
    "scaler = StandardScaler()\n",
    "X_structured_scaled = scaler.fit_transform(X_structured)\n",
    "\n",
    "print(f\"✓ Structured features prepared: {X_structured_scaled.shape}\")\n",
    "\n",
    "# Save scaler\n",
    "with open('feature_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Prepare targets\n",
    "y_classification = df_ml['Status_Binary'].values\n",
    "y_regression = df_ml['Enrollment_Log'].values\n",
    "\n",
    "print(f\"\\n[2.3] Dataset statistics:\")\n",
    "print(f\"  Total samples: {len(y_classification)}\")\n",
    "print(f\"  Classification target: {np.unique(y_classification, return_counts=True)}\")\n",
    "print(f\"  Regression target: min={y_regression.min():.2f}, max={y_regression.max():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SPLIT DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[2.4] Splitting data...\")\n",
    "\n",
    "# Classification split\n",
    "X_text_train, X_text_test, X_struct_train, X_struct_test, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_text, X_structured_scaled, y_classification, \n",
    "    test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "\n",
    "# Regression split\n",
    "_, _, _, _, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_text, X_structured_scaled, y_regression,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"✓ Train size: {len(X_text_train)}, Test size: {len(X_text_test)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: LSTM FOR TEXT CLASSIFICATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[MODEL 1] LSTM NEURAL NETWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[3.1] Building LSTM model...\")\n",
    "\n",
    "# LSTM model architecture\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "], name='LSTM_Classifier')\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(lstm_model.summary())\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "print(\"\\n[3.2] Training LSTM model...\")\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_text_train, y_train_clf,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n[3.3] Evaluating LSTM model...\")\n",
    "y_pred_lstm = (lstm_model.predict(X_text_test) > 0.5).astype(int).flatten()\n",
    "lstm_f1 = f1_score(y_test_clf, y_pred_lstm, average='weighted')\n",
    "\n",
    "print(f\"\\n✓ LSTM RESULTS:\")\n",
    "print(f\"  F1-Score: {lstm_f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_clf, y_pred_lstm, target_names=['Not Completed', 'Completed']))\n",
    "\n",
    "# Save model\n",
    "lstm_model.save('lstm_model.h5')\n",
    "print(\"✓ LSTM model saved as 'lstm_model.h5'\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: CNN FOR TEXT CLASSIFICATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[MODEL 2] CNN (Convolutional Neural Network)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[4.1] Building CNN model...\")\n",
    "\n",
    "# CNN model architecture\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "], name='CNN_Classifier')\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(cnn_model.summary())\n",
    "\n",
    "print(\"\\n[4.2] Training CNN model...\")\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_text_train, y_train_clf,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n[4.3] Evaluating CNN model...\")\n",
    "y_pred_cnn = (cnn_model.predict(X_text_test) > 0.5).astype(int).flatten()\n",
    "cnn_f1 = f1_score(y_test_clf, y_pred_cnn, average='weighted')\n",
    "\n",
    "print(f\"\\n✓ CNN RESULTS:\")\n",
    "print(f\"  F1-Score: {cnn_f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_clf, y_pred_cnn, target_names=['Not Completed', 'Completed']))\n",
    "\n",
    "# Save model\n",
    "cnn_model.save('cnn_model.h5')\n",
    "print(\"✓ CNN model saved as 'cnn_model.h5'\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: HYBRID MODEL (TEXT + STRUCTURED FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[MODEL 3] HYBRID DEEP LEARNING MODEL (Text + Structured)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[5.1] Building hybrid model...\")\n",
    "\n",
    "# Text input branch\n",
    "text_input = Input(shape=(MAX_LEN,), name='text_input')\n",
    "text_embedding = Embedding(input_dim=MAX_WORDS, output_dim=128)(text_input)\n",
    "text_lstm = Bidirectional(LSTM(64, return_sequences=True))(text_embedding)\n",
    "text_lstm = Dropout(0.5)(text_lstm)\n",
    "text_lstm = Bidirectional(LSTM(32))(text_lstm)\n",
    "text_features = Dropout(0.5)(text_lstm)\n",
    "\n",
    "# Structured input branch\n",
    "struct_input = Input(shape=(X_structured_scaled.shape[1],), name='struct_input')\n",
    "struct_dense = Dense(64, activation='relu')(struct_input)\n",
    "struct_dense = BatchNormalization()(struct_dense)\n",
    "struct_dense = Dropout(0.3)(struct_dense)\n",
    "struct_features = Dense(32, activation='relu')(struct_dense)\n",
    "\n",
    "# Combine branches\n",
    "combined = Concatenate()([text_features, struct_features])\n",
    "combined = Dense(128, activation='relu')(combined)\n",
    "combined = Dropout(0.5)(combined)\n",
    "combined = Dense(64, activation='relu')(combined)\n",
    "combined = Dropout(0.3)(combined)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid', name='output')(combined)\n",
    "\n",
    "# Create model\n",
    "hybrid_model = Model(inputs=[text_input, struct_input], outputs=output, name='Hybrid_Model')\n",
    "\n",
    "hybrid_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(hybrid_model.summary())\n",
    "\n",
    "print(\"\\n[5.2] Training hybrid model...\")\n",
    "history_hybrid = hybrid_model.fit(\n",
    "    [X_text_train, X_struct_train],\n",
    "    y_train_clf,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n[5.3] Evaluating hybrid model...\")\n",
    "y_pred_hybrid = (hybrid_model.predict([X_text_test, X_struct_test]) > 0.5).astype(int).flatten()\n",
    "hybrid_f1 = f1_score(y_test_clf, y_pred_hybrid, average='weighted')\n",
    "\n",
    "print(f\"\\n✓ HYBRID MODEL RESULTS:\")\n",
    "print(f\"  F1-Score: {hybrid_f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_clf, y_pred_hybrid, target_names=['Not Completed', 'Completed']))\n",
    "\n",
    "# Save model\n",
    "hybrid_model.save('hybrid_model.h5')\n",
    "print(\"✓ Hybrid model saved as 'hybrid_model.h5'\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 4: FEEDFORWARD NN FOR REGRESSION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[MODEL 4] FEEDFORWARD NEURAL NETWORK (Enrollment Prediction)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[6.1] Building regression model...\")\n",
    "\n",
    "# Regression model with structured features only\n",
    "regression_model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_structured_scaled.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Linear output for regression\n",
    "], name='Regression_NN')\n",
    "\n",
    "regression_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    ")\n",
    "\n",
    "print(regression_model.summary())\n",
    "\n",
    "print(\"\\n[6.2] Training regression model...\")\n",
    "history_reg = regression_model.fit(\n",
    "    X_struct_train, y_train_reg,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n[6.3] Evaluating regression model...\")\n",
    "y_pred_reg_nn = regression_model.predict(X_struct_test).flatten()\n",
    "nn_r2 = r2_score(y_test_reg, y_pred_reg_nn)\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_nn))\n",
    "\n",
    "print(f\"\\n✓ REGRESSION NN RESULTS:\")\n",
    "print(f\"  R² Score: {nn_r2:.4f}\")\n",
    "print(f\"  RMSE: {nn_rmse:.4f}\")\n",
    "\n",
    "# Save model\n",
    "regression_model.save('regression_nn_model.h5')\n",
    "print(\"✓ Regression NN model saved as 'regression_nn_model.h5'\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARE ALL MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load baseline results from previous step\n",
    "baseline_f1 = 0.8006\n",
    "baseline_r2 = 0.2228\n",
    "xgboost_f1 = 0.8146\n",
    "xgboost_r2 = 0.2706\n",
    "\n",
    "print(\"\\n[7.1] CLASSIFICATION RESULTS (F1-Score):\")\n",
    "results_clf = pd.DataFrame({\n",
    "    'Model': ['Baseline RF', 'XGBoost', 'LSTM (DL)', 'CNN (DL)', 'Hybrid (DL)'],\n",
    "    'F1-Score': [baseline_f1, xgboost_f1, lstm_f1, cnn_f1, hybrid_f1],\n",
    "    'Type': ['ML', 'ML', 'DL', 'DL', 'DL']\n",
    "})\n",
    "results_clf = results_clf.sort_values('F1-Score', ascending=False)\n",
    "print(results_clf.to_string(index=False))\n",
    "\n",
    "print(\"\\n[7.2] REGRESSION RESULTS (R² Score):\")\n",
    "results_reg = pd.DataFrame({\n",
    "    'Model': ['Baseline RF', 'XGBoost', 'Feedforward NN (DL)'],\n",
    "    'R² Score': [baseline_r2, xgboost_r2, nn_r2],\n",
    "    'RMSE': [1.4767, 1.4306, nn_rmse],\n",
    "    'Type': ['ML', 'ML', 'DL']\n",
    "})\n",
    "results_reg = results_reg.sort_values('R² Score', ascending=False)\n",
    "print(results_reg.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n[7.3] Creating comparison visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Classification F1-Score Comparison\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['steelblue' if t == 'ML' else 'coral' for t in results_clf['Type']]\n",
    "bars = ax1.barh(range(len(results_clf)), results_clf['F1-Score'], color=colors, edgecolor='black', alpha=0.8)\n",
    "ax1.set_yticks(range(len(results_clf)))\n",
    "ax1.set_yticklabels(results_clf['Model'])\n",
    "ax1.set_xlabel('F1-Score', fontweight='bold')\n",
    "ax1.set_title('Classification Performance Comparison', fontweight='bold', fontsize=12)\n",
    "ax1.axvline(x=baseline_f1, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(results_clf.iterrows()):\n",
    "    ax1.text(row['F1-Score'], i, f\" {row['F1-Score']:.4f}\", \n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "# 2. Regression R² Comparison\n",
    "ax2 = axes[0, 1]\n",
    "colors_reg = ['steelblue' if t == 'ML' else 'coral' for t in results_reg['Type']]\n",
    "bars = ax2.barh(range(len(results_reg)), results_reg['R² Score'], color=colors_reg, edgecolor='black', alpha=0.8)\n",
    "ax2.set_yticks(range(len(results_reg)))\n",
    "ax2.set_yticklabels(results_reg['Model'])\n",
    "ax2.set_xlabel('R² Score', fontweight='bold')\n",
    "ax2.set_title('Regression Performance Comparison', fontweight='bold', fontsize=12)\n",
    "ax2.axvline(x=baseline_r2, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(results_reg.iterrows()):\n",
    "    ax2.text(row['R² Score'], i, f\" {row['R² Score']:.4f}\", \n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "# 3. Training History - Hybrid Model\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history_hybrid.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax3.plot(history_hybrid.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax3.set_xlabel('Epoch', fontweight='bold')\n",
    "ax3.set_ylabel('Loss', fontweight='bold')\n",
    "ax3.set_title('Hybrid Model Training History', fontweight='bold', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. ML vs DL Comparison\n",
    "ax4 = axes[1, 1]\n",
    "ml_avg = results_clf[results_clf['Type'] == 'ML']['F1-Score'].mean()\n",
    "dl_avg = results_clf[results_clf['Type'] == 'DL']['F1-Score'].mean()\n",
    "\n",
    "categories = ['Machine Learning\\n(Avg)', 'Deep Learning\\n(Avg)']\n",
    "values = [ml_avg, dl_avg]\n",
    "colors_comp = ['steelblue', 'coral']\n",
    "\n",
    "bars = ax4.bar(categories, values, color=colors_comp, edgecolor='black', alpha=0.8, width=0.6)\n",
    "ax4.set_ylabel('Average F1-Score', fontweight='bold')\n",
    "ax4.set_title('ML vs DL Average Performance', fontweight='bold', fontsize=12)\n",
    "ax4.set_ylim([0.75, 0.85])\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dl_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved as 'dl_model_comparison.png'\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[SUMMARY] DEEP LEARNING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_clf_model = results_clf.iloc[0]\n",
    "best_reg_model = results_reg.iloc[0]\n",
    "\n",
    "summary = f\"\"\"\n",
    "DEEP LEARNING MODELS TRAINED:\n",
    "\n",
    "1. LSTM (Bidirectional):\n",
    "   • Architecture: Embedding → Bi-LSTM (64) → Bi-LSTM (32) → Dense\n",
    "   • F1-Score: {lstm_f1:.4f}\n",
    "   • Best for: Sequential text patterns\n",
    "\n",
    "2. CNN (Convolutional):\n",
    "   • Architecture: Embedding → Conv1D → GlobalMaxPool → Dense\n",
    "   • F1-Score: {cnn_f1:.4f}\n",
    "   • Best for: Local text patterns, faster training\n",
    "\n",
    "3. Hybrid Model (LSTM + Structured):\n",
    "   • Architecture: Text branch + Structured branch → Combined\n",
    "   • F1-Score: {hybrid_f1:.4f}\n",
    "   • Best for: Combining text and tabular data\n",
    "\n",
    "4. Feedforward NN (Regression):\n",
    "   • Architecture: Dense layers with BatchNorm and Dropout\n",
    "   • R² Score: {nn_r2:.4f}\n",
    "   • RMSE: {nn_rmse:.4f}\n",
    "\n",
    "OVERALL BEST MODELS:\n",
    "  Classification: {best_clf_model['Model']} (F1: {best_clf_model['F1-Score']:.4f})\n",
    "  Regression: {best_reg_model['Model']} (R²: {best_reg_model['R² Score']:.4f})\n",
    "\n",
    "KEY INSIGHTS:\n",
    "  • Deep Learning achieves competitive performance with ML\n",
    "  • Hybrid models leverage both text and structured data\n",
    "  • LSTM captures sequential dependencies in clinical text\n",
    "  • CNN provides faster training with good performance\n",
    "  • Neural networks benefit from large vocabulary size\n",
    "\n",
    "COMPARISON WITH BASELINE:\n",
    "  • Baseline F1: {baseline_f1:.4f}\n",
    "  • Best DL F1: {max(lstm_f1, cnn_f1, hybrid_f1):.4f}\n",
    "  • Improvement: {((max(lstm_f1, cnn_f1, hybrid_f1) - baseline_f1)/baseline_f1*100):.2f}%\n",
    "\n",
    "FILES SAVED:\n",
    "  • lstm_model.h5\n",
    "  • cnn_model.h5\n",
    "  • hybrid_model.h5\n",
    "  • regression_nn_model.h5\n",
    "  • text_tokenizer.pkl\n",
    "  • feature_scaler.pkl\n",
    "  • dl_model_comparison.png\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEEP LEARNING STEP COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou now have both ML and DL models for comparison!\")\n",
    "print(\"Proceed to Step 5 for final interpretation including DL results.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8429078,
     "sourceId": 13298760,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8429085,
     "sourceId": 13298769,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8429156,
     "sourceId": 13298854,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 143364534,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 904.901055,
   "end_time": "2025-10-08T12:56:57.173677",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-08T12:41:52.272622",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
